{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA3\n",
    "\n",
    "In this assignment, you will perform logistic regression on the MNIST dataset to detect which number is written in an image! Each image in the MNIST data shows a handwritten number, and your goal is to use logistic regression to detect when a number is an \"8\" or a \"1\".\n",
    "\n",
    "Each input example ($x_i$ from class) is a vector referring to the brightness of each pixel in an image. Your logistic regression will learn a weight vector whose length is equal to the number of pixels in each image.\n",
    "\n",
    "First, you will train logistic regression to recognize the digit 8. You'll do this once with coordinate descent (CD) and once with SGD (discussed in class). Then, you'll train logistic regression to recognize the digit 1, using your preferred method between CD and SGD.\n",
    "\n",
    "Fill in the ACT's below. Writing asserts to test your code is recommended, but not graded for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Tensorflow before calling the following line (for anaconda, run \"conda install tensorflow\")\n",
    "# (for a native Python installation, you can try pip or pip3 install tensorflow)\n",
    "\n",
    "# Import the MNIST data\n",
    "from tensorflow.keras.datasets import mnist as keras_mnist\n",
    "(X_train, y_train), (X_test, y_test) = keras_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (60000, 28, 28)\n",
      "Shape of X_test:  (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# There are 60000 training examples, each example is a 28 by 28 pixel\n",
    "#   grayscale image, represented by a 28 by 28 array of numbers\n",
    "print('Shape of X_train: ', X_train.shape)\n",
    "print('Shape of X_test: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-Loss and Its Gradient\n",
    "\n",
    "In lecture, we used the convention that negative examples are labeled $-1$ and positive examples are labeled $+1$. That is, $y_i\\in\\{-1,+1\\}$. We found that, if $\\mathcal{L}(\\mathbf{w})$ is our loss on $\\mathbf{w}$, and $|S|$ is the number of examples,\n",
    "$$\\mathcal{L}(\\mathbf{w}) = \\frac{1}{|S|} \\sum_{i\\in S} \\log\\big(1+e^{-z_i}\\big)\n",
    "~~ \\mbox{ where } ~~\n",
    "z_i = y_i (\\mathbf{w} \\cdot \\mathbf{x}_i) ~~ .$$\n",
    "This occurs when we let $\\mathcal{L}(\\mathbf{w})$ be the negative log probability of us predicting the right label. Note: Sometimes, in lecture we used $\\mathcal{L}(\\mathbf{w}) = \\sum_{i\\in S} \\log(1+e^{-z_i})$, omitting the division by $|S|$. Since we generally assume that the number of examples $|S|$ is fixed, these two forms of the loss differ by a constant factor, so they are pretty much equivalent.\n",
    "\n",
    "Alternatively, suppose we use a different convention, that negative examples are labeled $0$ instead. That is, $y_i\\in\\{0,1\\}$. Just as in the previous case, we predict $+1$ with probability $\\hat{y}_i = 1/(1+e^{-\\mathbf{w} \\cdot \\mathbf{x_i}})$. Thus, we predict $0$ with probability $1 - \\hat{y}_i = 1 - 1/(1+e^{-\\mathbf{w} \\cdot \\mathbf{x_i}}) = 1/(1+e^{\\mathbf{w} \\cdot \\mathbf{x_i}})$.\n",
    "\n",
    "Then, observe that the probability of predicting the right label is equal to $\\hat{y}_i$ when $y_i = 1$, and is equal to $1 - \\hat{y}_i$ when $y_i = 0$. Then, do you see why the loss can be expressed as the following? (You can plug in $y_i = 1$ or $y_i = 0$.)\n",
    "$$\\mathcal{L}(\\mathbf{w}) =\n",
    "  -\\frac{1}{|S|} \\sum_{i\\in S} \\big(y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)\\big)\n",
    "~~ \\mbox{ where } ~~ \n",
    "\\hat{y}_i = \\frac{1}{1+e^{-\\mathbf{w} \\cdot \\mathbf{x_i}}}\n",
    "~~.$$\n",
    "\n",
    "If we find the gradient for the first form (when $y_i\\in\\{-1,+1\\}$), we obtain the following:\n",
    "$$ \\nabla_{\\mathbf{w}} \\mathcal{L} = -\\frac{1}{|S|} \\sum_{i\\in S} q_i y_i \\mathbf{x}_i \n",
    "~~ \\mbox{ where } ~~ q_i = \\frac{1}{1 + e^{z_i}} ~~ .\n",
    "$$\n",
    "\n",
    "For the second form (when $y_i\\in\\{0,+1\\}$), we find that the gradient is:\n",
    "$$ \\nabla_{\\mathbf{w}} \\mathcal{L} = \\frac{1}{|S|} \\sum_{i\\in S} \\big(\\hat{y}_i - y_i\\big) \\mathbf{x}_i  ~~ ,$$\n",
    "where $\\hat{y}_i$ is as defined above. You can verify these gradient values for yourself.\n",
    "\n",
    "For your own practice (ungraded), you should confirm the equivalence of these two forms, and derive them for yourself. That is, calculate both gradients (see if you get the same thing we did). Then, verify that when you plug in $y_i = 1$ you get the same thing, and when you plug in $y_i = -1$ or $=0$ respectively, you get the same thing. If you're having trouble with this, please ask on Piazza!\n",
    "\n",
    "Finally, implement the following functions for the case where $y_i\\in\\{-1,+1\\}$. You will use these functions throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Calculate loss from margins\n",
    "# Here, z is equal to y times X dot w\n",
    "# That is, z_i = y_i * x_i dot w\n",
    "def logloss_from_z(z):\n",
    "    ### ACT 1\n",
    "    l = [np.log(1 + math.exp(-z_i)) for z_i in z]\n",
    "    return np.average(l)\n",
    "    \n",
    "\n",
    "# Calculate error from margins\n",
    "# the error is the proportion of examples that you \"get wrong\".\n",
    "# a.k.a. the examples where you assign less than 50% chance to the correct label\n",
    "# This is equivalent to the proportion of examples where z_i is less than 0\n",
    "def error_from_z(z):\n",
    "    ### ACT 2\n",
    "    count = np.sum(np.array(z) < 0, axis=0)\n",
    "    return count[0]/len(z)\n",
    "    \n",
    "\n",
    "# Calculate loss from parameter vector\n",
    "def logloss(X, y, w):\n",
    "    ### ACT 3\n",
    "    z = y*np.dot(X,w)\n",
    "    return logloss_from_z(z)\n",
    "    \n",
    "\n",
    "# Calculate error from parameter vector\n",
    "def error(X, y, w):\n",
    "    ### ACT 4\n",
    "    z = y*np.dot(X,w)\n",
    "    return error_from_z(z)\n",
    "    \n",
    "\n",
    "# Gradient of LogLoss w.r.t. w\n",
    "def logloss_gradient(X, y, w):\n",
    "    ### ACT 5\n",
    "    z = y*np.dot(X,w)\n",
    "    q = [1/(1+math.exp(z_i)) for z_i in z] #vector of q's\n",
    "\n",
    "    # column-wise multiplication of example matrix with the vector of constants\n",
    "    # i.e. each row = (q_i * y_i) * X_row_i\n",
    "    # results in a nxd matrix\n",
    "    L_grad = (X.T * (q*y)).T\n",
    "    #print(L_grad)\n",
    "    \n",
    "    L_grad = np.sum(L_grad,axis=0) # sum across the columns to find total gradient in each parameter\n",
    "    L_grad_normed = -L_grad/len(y) # normalize, divide each column sum by -1/sample_size\n",
    "    \n",
    "    #print(L_grad)\n",
    "    #print(L_grad_normed)\n",
    "    return L_grad_normed\n",
    "    \n",
    "\n",
    "# Calculate error & loss from parameter vector\n",
    "# Return two floats in the format (loss, error)\n",
    "def logloss_and_error(X, y, w):\n",
    "    ### ACT 6\n",
    "    l = logloss(X, y, w)\n",
    "    e = error(X,y,w)\n",
    "    return l,e\n",
    "    \n",
    "\n",
    "#ASSERTS\n",
    "assert(round(logloss_from_z(np.array([0,2,4]).reshape(-1,1)),15) == 0.2794083731735760)\n",
    "assert(error_from_z(np.array([0,-2,4,1]).reshape(-1,1)) == 1/4)\n",
    "\n",
    "X = np.array([[1, 2, 3], [3, 4, 3], [1, 3, 5]])\n",
    "y = np.array([1, -1, 1]).reshape(-1,1)\n",
    "w = np.array([0.1, 0.2, 0.1]).reshape(-1,1)\n",
    "\n",
    "assert(round(logloss(X,y,w),15) == 0.7516001810680870)\n",
    "assert(error(X,y,w) == 1/3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "Normally, we initialize $w$ to be zero (as we discussed in the last assignment). This means we initially predict everything as 0.5 (a.k.a. 50-50). This might not make sense. For example, in the MNIST problem, we are distinguishing a single digit from the 9 other digits. On average, there's only a 10% chance each image is our desired digit, so initially maybe we should predict something lower.\n",
    "\n",
    "This is where bias comes in. Bias is something that is added to every prediction. For example, if we have bias $b$, an example $x_i$, and a weight vector $\\mathbf{w}$, we would predict the following probability:\n",
    "$$\\Pr[y_i = +1] = \\frac{1}{1 + e^{-(\\mathbf{w}\\cdot \\mathbf{x}_i + b)}}$$\n",
    "\n",
    "So, in MNIST, we'd want to initialize bias to be negative, since initially we would predict that an average digit should have less than 50% chance to be classified as $+1$.\n",
    "\n",
    "Bias is actually easy to add into our models.\n",
    "\n",
    "Rather than creating a new variable $b$, we simply append $1$ to each example $\\mathbf{x}_i$. (That is, we append a column of ones to the data matrix $X$.) Since the length of $\\mathbf{x}_i$ has increased by one, the length of our weight vector $\\mathbf{w}$ has also increased by one. Let's call the final entry of our weight vector $w_d$, and call the final entry of our example $x_{i, d}$ (so $x_{i, d} = 1$). Then, when we compute $\\mathbf{w}\\cdot \\mathbf{x}_i$, we get\n",
    "$$\\mathbf{w} \\cdot \\mathbf{x}_i = w_1x_{i, 1} + w_2x_{i, 2} + \\ldots + w_dx_{i, d}$$\n",
    "which equals the previous dot product, plus $w_dx_{i, d}$ which equals $w_d$.\n",
    "\n",
    "Thus, $w_{d}$ becomes the bias term: it is added to every dot product regardless of the example $\\mathbf{x}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Initial Bias Using Log-Odds (ACT's 7-9)\n",
    "As mentioned above, we don't necessarily want to initialize the bias term of $w$, $w_{d}$, to zero. Below is a small math exercise where you will find the best value to initialize bias.\n",
    "\n",
    "Total number of positive examples is $n_+$ (pos) and negative $n_-$ (neg). Assume $\\mathbf{w}=\\mathbf{0}$ except for the weight of the last feature $w_{d}$. Since we have the other entries of $w$ initialized equal to $0$, we can write the logistic loss as the following:\n",
    "$$\\frac1n \\sum_{i:y_i=+1} \\log(1+\\exp(-w_d)) + \\frac1n \\sum_{i:y_i=-1} \\log(1+\\exp(w_d))$$\n",
    "If you're unclear on how we got this, verify it by plugging in $\\mathbf{w}$ into the formula for logistic loss. Most of the terms in the dot product equal zero except for the last term in the dot product. For reference: $\\exp(-w_d) = e^{-w_d}$.\n",
    "\n",
    "To find the best initialization for $w_d$, we take the derivative of the above loss, and set it to $0$. The ACT's below ask you to do this, to find the best initial bias. Submit your answer as a LaTeX'd answer within this notebook cell (or make a new notebook cell).\n",
    "\n",
    "ACT 7: Take the derivative, with respect to $w_{d}$, of the above expression for logistic loss.\n",
    "\n",
    "ACT 8: Set the derivative of this to zero, and solve for w_d.\n",
    "\n",
    "ACT 9: Fill in the value of $w_d$ below which minimizes your initial loss, then code it below. (We've given you a hint, the answer is a log of something.)\n",
    "$$\n",
    "w_d = \\log\\left( \\mathbf{\\mbox{ACT 9}} \\right) ~.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACT 7:\n",
    "\n",
    "$$ \\frac{dy}{dw_d} = \\frac{n_+}{n}\\cdot\\frac{-e^{-w_d}}{1+e^{-w_d}} + \\frac{n_-}{n}\\cdot\\frac{e^{w_d}}{1+e^{w_d}} $$\n",
    "\n",
    "where:\n",
    "\n",
    "$n_{+}$ = number of positive labels\n",
    "\n",
    "$n_{-}$ = number of negative labels\n",
    "\n",
    "\n",
    "### simplify to:\n",
    "\n",
    "$$ \\frac{dy}{dw_d} = \\frac{-n_{+} + n_{-}\\cdot e^{w_d}}{n(1+e^{w_d})} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACT 8:\n",
    "\n",
    "$$ \\frac{dy}{dw_d} = \\frac{-n_{+} + n_{-}\\cdot e^{w_d}}{n(1+e^{w_d})} = 0$$\n",
    "\n",
    "$$ e^{w_d} = \\frac{n_{+}}{n_{-}}$$\n",
    "\n",
    "$$ w_d = log\\left(\\frac{n_{+}}{n_{-}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the initial bias\n",
    "def init_bias(pos, neg):\n",
    "    ### ACT 9 continued\n",
    "    return pos/neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image normalization and flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working with MNIST, we need to normalize and flatten the input images. After all, we do not normally perform logistic regression on 2D input data like images, we normally perform logistic regression on 1D data! So, we first flatten the image so it becomes 1-dimensional.\n",
    "\n",
    "First we flatten the data set of images: each $p_x \\times p_y$ image becomes a 1d vector of size $d = p_x \\, p_y$.\n",
    "\n",
    "This amounts to reshaping $X$ from a $n \\times p_x \\times p_y$ tensor to a $n \\times d$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_images(X):\n",
    "    ### ACT 10\n",
    "    X_reshaped = np.zeros((X.shape[0],X.shape[1]*X.shape[2]))\n",
    "    \n",
    "    for i,image in enumerate(X):\n",
    "        image_vec = image.ravel()\n",
    "        X_reshaped[i,:] = image_vec.reshape(1,-1)\n",
    "        \n",
    "    return(X_reshaped)\n",
    "    \n",
    "#ASSERTS\n",
    "X = np.zeros((3,4,4))\n",
    "X[0][1][1] = 4\n",
    "X[0][3][3] = 7\n",
    "\n",
    "X[1][0][1:3] = 3\n",
    "X[1][3][1] = 2\n",
    "\n",
    "X[2][2][1:5] = 7\n",
    "X[2][3][1] = 2\n",
    "\n",
    "a = flatten_images(X)\n",
    "assert(a.shape[0] == X.shape[0])\n",
    "assert(a.shape[1] == X.shape[1]*X.shape[2])\n",
    "assert(a[0][5] == 4)\n",
    "assert(a[1][13] == 2)\n",
    "assert(a[2][9] == 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to normalize each image. This is useful because some images may be overall darker or lighter, or higher- or lower-contrast than others. We don't want this to affect our classification. Thus, we normalize each image by making sure each image's average \"brightness\" (a.k.a. pixel value) is 0, and each image's pixels have standard deviation one.\n",
    "\n",
    "Finally, we want to provide the option to append a bias column of ones to the data matrix.\n",
    "\n",
    "In your method below, normalize each image, by subtracting the average value of that image over all its pixels, and\n",
    "dividing by the standard deviation of the image's pixel values. If the bias argument is non-zero, add a bias term by appending `bias` to each example. For example, if `bias=1`, then append an entry of $1$ to each example. Algebraicly, flattening and normalizing amounts to flattening each image to a $d$ dimensional vector\n",
    "$\\mathbf{x}$ and calculating average pixel value and variance of pixel values,\n",
    "$$ m = \\mathbb{E}(\\mathbf{x}) = \\frac 1 d \\sum_{i=1}^d x_i ~~~\n",
    "   s^2 = \\mathbb{V}(\\mathbf{x}) = \\frac 1 d \\sum_{i=1}^d x_i^2 - m^2 ~~ . $$\n",
    "Here, $m$ is the image's mean and $s$ is the image's standard deviation. (You can use pre-existing numpy functions to compute these.)\n",
    "\n",
    "Define $a=s^{-1}$, then the normalized image (represented as a vector) is, ${a (\\mathbf{x} - m)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each example as described above. If bias is non-zero, add a bias term by appending `bias` to\n",
    "# each example\n",
    "def normalize(X, bias=0):\n",
    "    ### ACT 11\n",
    "    m = X.mean(1).reshape(-1,1)\n",
    "    s = np.std(X, axis=1).reshape(-1,1)\n",
    "    \n",
    "    a = 1/s\n",
    "    \n",
    "    X_normed = a*(X - m)\n",
    "    \n",
    "    if bias:\n",
    "        #Add column of 'bias' at the end\n",
    "        col = np.arange(X_normed.shape[0])\n",
    "        col = np.full_like(col,bias).reshape(-1,1)\n",
    "        X_normed = np.append(X_normed, col, axis=1)\n",
    "    \n",
    "    return X_normed\n",
    "\n",
    "#ASSERTS\n",
    "X = np.zeros((3,4,4))\n",
    "X[0][1][1] = 4\n",
    "X[0][3][3] = 7\n",
    "\n",
    "X[1][0][1:3] = 3\n",
    "X[1][3][1] = 2\n",
    "\n",
    "X[2][2][1:5] = 7\n",
    "X[2][3][1] = 2\n",
    "\n",
    "a = flatten_images(X)\n",
    "\n",
    "#Without Bias\n",
    "a_normed = normalize(a,0)\n",
    "assert(a_normed.mean(1).reshape(-1,1).sum() < 0.0000000000001) #Check mean ~0\n",
    "assert(a_normed.shape[1] == a.shape[1]) #Check correct number of columns\n",
    "\n",
    "#With Bias\n",
    "bias = 2\n",
    "a_normed = normalize(a,bias)\n",
    "assert(a_normed[0][-1] == bias) #Check last number = bias\n",
    "assert(a_normed.shape[1] == a.shape[1]+1) #Check that added column\n",
    "\n",
    "#print(X_train[0:2])\n",
    "#normalize(flatten_images(X_train[0:2]))\n",
    "\n",
    "#print(y_train[0:20])\n",
    "#print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and flatten the training and test images\n",
    "# You may print Xtr and Xte, or view their shape, to get a feeling for their structure\n",
    "Xtr = normalize(flatten_images(X_train), bias=1)\n",
    "Xte = normalize(flatten_images(X_test), bias=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the train labels $\\mathbf{y}^{\\mbox{tr}}$ and test labels $\\mathbf{y}^{\\mbox{te}}$ to vectors over\n",
    "$\\{-1,+1\\}$ indicating whether each example is an 8.\n",
    "\n",
    "For example, `ytr8[i] = -1` if the i-th example is NOT an 8\n",
    "\n",
    "Similarly to above, viewing the first few entries of ytr8,\n",
    "may give you some intuition about their contents and structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr8 = (2 * (y_train == 8) - 1).reshape(len(y_train), 1)\n",
    "yte8 = (2 * (y_test == 8) - 1).reshape(len(y_test), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACT 12: For coordinate descent use the mean value theorem to form a quadratic upper-bound on the loss for a single coordinate $j$. When using the convention of $y_i\\in\\{0,1\\}$ show that,\n",
    "    $$\n",
    "     |S|\\cdot\\mathcal{L}(\\mathbf{w}+\\delta\\mathbf{1}_j) \\leq\n",
    "       \\kappa + \\sum_i (\\hat{y}_i - y_i) X_{ij} \\, \\delta + \\frac18 \\sum_i  X_{ij}^2 \\, \\delta^2 ~~.\n",
    "    $$\n",
    "Here we use $\\mathbf{w}+\\delta\\mathbf{1}_j$ to denote $\\mathbf{w}$ plus $\\delta$ in the $j$-th coordinate. Hint: The slides titled \"Mean Value Theorem\" and \"Putting It All Together\" in the Logistic Regression lecture should be helpful. Please ask on Piazza if you're having trouble understanding ACT 12 or the relevant lecture slides!\n",
    "\n",
    "Note: The factor of $|S|$ multiplied on the left side is used to make this equation similar to the one proved in lecture. In this notebook we divided error and loss by $|S|$ to get average error or loss. However, in the logistic regression lecture we tried to minimize total loss (a.k.a. we didn't divide loss by $|S|$). Minimizing these two losses _is equivalent_ since $|S|$ is just a constant factor, so we multiply by $|S|$ on the left to keep things consistent with lecture in this ACT.\n",
    "\n",
    "ACT 13: Denote the $j^{\\mbox{th}}$ column of $X$ by $\\mathbf{v}_j$ and let $c_j= \\|\\mathbf{v}_j\\|^2$.\n",
    "\n",
    "Show that the $\\delta^\\star$ minimizing the above bound $\\kappa + \\sum_i (\\hat{y}_i - y_i) X_{ij} \\, \\delta + \\frac18 \\sum_i  X_{ij}^2 \\, \\delta^2$ is,\n",
    "$$\n",
    "\\delta^\\star = \\frac{4}{c_j} (\\mathbf{y - \\hat{y}) \\cdot v_j} = \\frac{4}{c_j} \\sum_{i=1}^n (y_i - \\hat{y}_i)\\mathbf{v}_j[i] ~~.\n",
    "$$\n",
    "\n",
    "(Note that $\\mathbf{v}_j[i] = X_{ij}$.) Hint: Take the derivative and set it equal to zero.\n",
    "\n",
    "ACT 14: Of course the above expression assumes that $y_i\\in \\{0, 1\\}$, which is different from our current setting, where $y_i \\in \\{-1, 1\\}$.\n",
    "\n",
    "To convert back to the $\\{-1, 1\\}$ setting, consider the positive and negative examples in the above summation. Positive examples have $y_i = 1$, so that term in the summation becomes $(1 - \\hat{y}_i)\\mathbf{v}_j[i]$. Negative examples have $y_i = 0$, so that term in the summation becomes $(-\\hat{y}_i)\\mathbf{v}_j[i]$.\n",
    "\n",
    "Can you rewrite the expression for $\\delta^\\star$ when $y_i\\in\\{-1, 1\\}$?\n",
    "\n",
    "Hint: First, re-write $(y_i - \\hat{y}_i)$ from the previous setting in terms of the new $y_i$ and $z_i$, or $y_i$ and $q_i$. You should get a simple expression that replaces $(y_i - \\hat{y}_i)$. The other terms in the expression for $\\delta^\\star$ shouldn't change, since they don't depend on $y_i$. Thus, you can just plug in your new expression for $(y_i - \\hat{y}_i)$ into the previous expression for $\\delta^\\star$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `x` is the j-th column of X (a.k.a. the v_j from above)\n",
    "# `y` is the vector of labels\n",
    "# `z` is a vector with entries z_i\n",
    "# `cj` is c_j from above, the squared norm of `x`\n",
    "def delta_wj(x, y, z, cj):\n",
    "    ### ACT 15 (code your result from ACT 14 here)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "from numpy.random import permutation\n",
    "\n",
    "# You don't need to use this class yourself: we provided the index-sampling code for you\n",
    "# sample new index (with or without replacement)\n",
    "# d is the max index\n",
    "class IndexSampler:\n",
    "    def __init__(self, d):\n",
    "        self.d = d\n",
    "        self.prm = None\n",
    "    \n",
    "    def sample_new_index(self, replace = 1):\n",
    "        if replace:\n",
    "            return randint(self.d)\n",
    "        if self.prm is None:\n",
    "            self.prm = permutation(self.d)\n",
    "            self.head = 0\n",
    "        ind = self.prm[self.head]\n",
    "        self.head += 1\n",
    "        if self.head == self.d:\n",
    "            self.head = 0\n",
    "            self.prm = None\n",
    "        return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs is maximum number of epochs to train\n",
    "# eps is your termination condition number, similar to in PA2's linear regression\n",
    "# Every epoch, report the loss (we've provided some code that reports loss for you)\n",
    "# (An epoch consists of d updates)\n",
    "\n",
    "### ACT 16\n",
    "def logistic_regression_cd(X, y, epochs=100, eps=0.001):\n",
    "    pstr = 'Epoch: {0:2d}  Loss: {1:5.3f}  Error: {2:5.3f}'\n",
    "    n, d = X.shape\n",
    "    ### initialize w, z, c, errors, and losses\n",
    "    ### c is a vector whose j-th entry is equal to c_j from above\n",
    "    ### errors should be a vector containing the error at each step\n",
    "    ### losses should be a vector containing the loss at each step\n",
    "    ### NOTE: Don't forget to initialize the last entry of w as initial bias!\n",
    "    \n",
    "    # Initialize 'w'\n",
    "    w = np.zeros((X.shape[1],1))\n",
    "    \n",
    "    neg = np.sum(y < 0, axis=0) # 5851 ~ 10%\n",
    "    pos = np.sum(y > 0, axis=0) # 54149 ~ 90%\n",
    "    \n",
    "    w[-1] = np.log(init_bias(pos,neg)) # bias = -2.22512692\n",
    "    \n",
    "    # Initialize 'z'\n",
    "    z = y*np.dot(X,w) # all values are magnitude of bias, since only bias term in w is not 0\n",
    "    \n",
    "    # Initialize 'c'\n",
    "    \n",
    "    # Initialize 'losses' and 'errors'\n",
    "    # errors[0] = pos/(pos+neg), since initial guess is that there are no 8's, i.e. w*x_i = negative\n",
    "    losses = []\n",
    "    errors = []\n",
    "    l,e = logloss_and_error(X,y,w)\n",
    "    \n",
    "    losses.append(l)\n",
    "    errors.append(e)\n",
    "    \n",
    "    cur_epoch = 0\n",
    "    sampler = IndexSampler(d)\n",
    "    for e in range(1, d * epochs + 1):\n",
    "        ### we've chosen a coordinate for you, now perform coordinate descent below\n",
    "        replace = 0 #me\n",
    "        j = sampler.sample_new_index(replace)\n",
    "        \n",
    "        if e % d == 0:\n",
    "            ### update losses and errors with the current loss and error\n",
    "            cur_epoch += 1\n",
    "            losses.append(0.3/cur_epoch) #placeholder\n",
    "            errors.append(0.10/cur_epoch) #placeholder\n",
    "            print(pstr.format(cur_epoch, losses[-1], errors[-1] * 100))\n",
    "            \n",
    "            if (losses[-2] - losses[-1]) / losses[-1] < eps: break\n",
    "        \n",
    "    print('\\n')\n",
    "    return w, losses, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3195920162418052]\n",
      "[0.09751666666666667]\n",
      "Epoch:  1  Loss: 0.300  Error: 10.000\n",
      "Epoch:  2  Loss: 0.150  Error: 5.000\n",
      "Epoch:  3  Loss: 0.100  Error: 3.333\n",
      "Epoch:  4  Loss: 0.075  Error: 2.500\n",
      "Epoch:  5  Loss: 0.060  Error: 2.000\n",
      "Epoch:  6  Loss: 0.050  Error: 1.667\n",
      "Epoch:  7  Loss: 0.043  Error: 1.429\n",
      "Epoch:  8  Loss: 0.037  Error: 1.250\n",
      "Epoch:  9  Loss: 0.033  Error: 1.111\n",
      "Epoch: 10  Loss: 0.030  Error: 1.000\n",
      "Epoch: 11  Loss: 0.027  Error: 0.909\n",
      "Epoch: 12  Loss: 0.025  Error: 0.833\n",
      "Epoch: 13  Loss: 0.023  Error: 0.769\n",
      "Epoch: 14  Loss: 0.021  Error: 0.714\n",
      "Epoch: 15  Loss: 0.020  Error: 0.667\n",
      "Epoch: 16  Loss: 0.019  Error: 0.625\n",
      "Epoch: 17  Loss: 0.018  Error: 0.588\n",
      "Epoch: 18  Loss: 0.017  Error: 0.556\n",
      "Epoch: 19  Loss: 0.016  Error: 0.526\n",
      "Epoch: 20  Loss: 0.015  Error: 0.500\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "[w, loss, err] = logistic_regression_cd(Xtr, ytr8, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss of your logistic classifier over time\n",
    "# The y-axis should be loss, and the x-axis should be the epoch\n",
    "# The plot doesn't need to be pretty, just show the loss going down over time!\n",
    "### ACT 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the error of your logistic classifier over time \n",
    "### ACT 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstr = 'Test  Loss: {0:5.3f}  Error: {1:5.3f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the loss and error of your weight vector on the test data and test labels\n",
    "# (Don't update your weight vector here: this is purely to evaluate it)\n",
    "test_loss, test_err = ### ACT 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pstr.format(test_loss, test_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we visualize w for you\n",
    "# First, we bound the entries of w within 3 standard deviations\n",
    "# (in order to ignore outliers which would mess up the image)\n",
    "image_w = np.maximum(np.minimum(w, 3 * np.std(w)), -3 * np.std(w))\n",
    "# Next, we re-shape image_w into the original 28 by 28 shape of the image\n",
    "image_w = image_w[0:-1].reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize image_w\n",
    "plt.axis('off')\n",
    "plt.imshow(image_w, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `ind` is the indices in Xtest of images which are \"8\" but classified incorrectly\n",
    "#   by your classifier\n",
    "ind = (np.argwhere(((yte8 == 1) * (np.matmul(Xte, w) < 0))))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 25 images of \"8\"s which your classifier incorrectly classified as non-8\n",
    "# This kind of visualization can be useful if you're wondering what sorts of images give\n",
    "#   your classifier trouble\n",
    "ncols, nrows = 5, 5\n",
    "fig, axes = plt.subplots(ncols, nrows, figsize=(1.5*ncols, 1.5*nrows))\n",
    "for i in range(ncols * nrows):\n",
    "    ax = axes[i//ncols, i%ncols]\n",
    "    x = X_test[ind[i],:,:].reshape(28,28)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    ax.imshow(x, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that you've completed coordinate descent, you will implement SGD!\n",
    "\n",
    "# A handle is a convenient way to pass in many arguments/specifications\n",
    "# to your function. In this case, the logistic SGD handle holds\n",
    "# parameters such as the gradient function, the loss function, and learning rate.\n",
    "\n",
    "### ACT 20\n",
    "# Fill in the missing parameters in the handle below, and experiment with\n",
    "# different values for them!\n",
    "# For the final submission, use the parameters you found worked best, except set eps = 0.001\n",
    "# (We will not be too picky with your parameters but we do expect them\n",
    "# to be good enough for your SGD to properly learn)\n",
    "# Instead of eta==0 use a real learning rate, (hint: try 0.01, 0.05, and 0.1)\n",
    "# Experiment also with a decreasing learning rate: eta[t] = eta / sqrt(c + t) for eta=1 and c=10\n",
    "def prepare_logistic_sgd_handle():\n",
    "    h = dict()\n",
    "    h['pstr'] = 'Epoch: {0:2d}  Loss: {1:5.3f}  Error: {2:5.3f}'\n",
    "    h['epochs'] = 100\n",
    "    # rather than a single value, eta is an array here, containing the eta for each epoch\n",
    "    h['eta'] = 0.00 * np.ones(h['epochs'])\n",
    "    h['grad'] = logloss_gradient\n",
    "    h['loss'] = logloss\n",
    "    h['error'] = error\n",
    "    ### Adjust this batch size if you wish. A size of 1000 should be fine, though.\n",
    "    h['batch_size'] = 1000\n",
    "    ### You can play with eps, but in your final submission set it to 0.001, so\n",
    "    ### you can achieve the desired accuracy.\n",
    "    h['eps'] = 0.001\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the termination condition (it can be similar to your\n",
    "#   termination condition for coordinate descent)\n",
    "# We recommend you don't terminate when c_loss is greater than p_loss, though\n",
    "def terminate(p_loss, c_loss, eps):\n",
    "    ### ACT 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 22\n",
    "# Implement logistic regression with SGD\n",
    "# h is the handle you defined above\n",
    "def sgd(X, y, h):\n",
    "    loss, error, grad, eta = h['loss'], h['error'], h['grad'], h['eta']\n",
    "    epochs, bs = h['epochs'], h['batch_size']\n",
    "    eps, pstr = h['eps'], h['pstr']\n",
    "    n, d = X.shape\n",
    "    nbs = int(n / bs)\n",
    "    sampler = IndexSampler(nbs)\n",
    "    ### ACT initialize w, losses, errors, and other variables you may need\n",
    "    for e in range(1, epochs * nbs):\n",
    "        # these two lines get a batch of examples and labels for you\n",
    "        head = sampler.sample_new_index(replace=0) * bs\n",
    "        Xt, yt = X[head:head + bs], y[head:head + bs]\n",
    "        ### ACT find stochastic gradient using the functions above with (Xt, yt)\n",
    "        ### ACT update w as appropriate\n",
    "        if e % nbs == 0:\n",
    "            ### ACT update losses and errors\n",
    "            print(pstr.format(e // nbs, losses[-1], errors[-1]))\n",
    "            if terminate(losses[-2], losses[-1], eps): break\n",
    "    print('\\n')\n",
    "    return w, losses, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w_sgd, loss_sgd, error_sgd] = sgd(Xtr, ytr8, prepare_logistic_sgd_handle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss of your SGD classifier over time\n",
    "### ACT 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the error of your SGD classifier over time\n",
    "### ACT 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the error of your SGD classifier and CD classifer on the same graph\n",
    "### ACT 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is similar to above\n",
    "ind = (np.argwhere(((yte8 == 1) * (np.matmul(Xte, w_sgd) < 0))))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 25 images of \"8\"s which your classifier incorrectly classified as non-8\n",
    "ncols, nrows = 5, 5\n",
    "fig, axes = plt.subplots(ncols, nrows, figsize=(1.5*ncols, 1.5*nrows))\n",
    "for i in range(ncols * nrows):\n",
    "    ax = axes[i//ncols, i%ncols]\n",
    "    x = X_test[ind[i],:,:].reshape(28,28)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    ax.imshow(x, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying the digit 1\n",
    "Now, repeat most of the above steps, except using the digit 1 instead. In particular:\n",
    "- Obtain training and test labels (yte1 and ytr1) (Xtr and Xte should remain the same)\n",
    "- Train a new weight vector using either CD or SGD\n",
    "- Visualize your weight vector for \"1\"\n",
    "- Plot the error of your weight vector for 1 over time\n",
    "- Display images of \"1\"s which your classifier incorrectly classified as non-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 26 (this may take multiple cells)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
