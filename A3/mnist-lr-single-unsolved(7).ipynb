{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA3\n",
    "\n",
    "In this assignment, you will perform logistic regression on the MNIST dataset to detect which number is written in an image! Each image in the MNIST data shows a handwritten number, and your goal is to use logistic regression to detect when a number is an \"8\" or a \"1\".\n",
    "\n",
    "Each input example ($x_i$ from class) is a vector referring to the brightness of each pixel in an image. Your logistic regression will learn a weight vector whose length is equal to the number of pixels in each image.\n",
    "\n",
    "First, you will train logistic regression to recognize the digit 8. You'll do this once with coordinate descent (CD) and once with SGD (discussed in class). Then, you'll train logistic regression to recognize the digit 1, using your preferred method between CD and SGD.\n",
    "\n",
    "Fill in the ACT's below. Writing asserts to test your code is recommended, but not graded for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Tensorflow before calling the following line (for anaconda, run \"conda install tensorflow\")\n",
    "# (for a native Python installation, you can try pip or pip3 install tensorflow)\n",
    "\n",
    "# Import the MNIST data\n",
    "from tensorflow.keras.datasets import mnist as keras_mnist\n",
    "(X_train, y_train), (X_test, y_test) = keras_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (60000, 28, 28)\n",
      "Shape of X_test:  (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# There are 60000 training examples, each example is a 28 by 28 pixel\n",
    "#   grayscale image, represented by a 28 by 28 array of numbers\n",
    "print('Shape of X_train: ', X_train.shape)\n",
    "print('Shape of X_test: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-Loss and Its Gradient\n",
    "\n",
    "In lecture, we used the convention that negative examples are labeled $-1$ and positive examples are labeled $+1$. That is, $y_i\\in\\{-1,+1\\}$. We found that, if $\\mathcal{L}(\\mathbf{w})$ is our loss on $\\mathbf{w}$, and $|S|$ is the number of examples,\n",
    "$$\\mathcal{L}(\\mathbf{w}) = \\frac{1}{|S|} \\sum_{i\\in S} \\log\\big(1+e^{-z_i}\\big)\n",
    "~~ \\mbox{ where } ~~\n",
    "z_i = y_i (\\mathbf{w} \\cdot \\mathbf{x}_i) ~~ .$$\n",
    "This occurs when we let $\\mathcal{L}(\\mathbf{w})$ be the negative log probability of us predicting the right label. Note: Sometimes, in lecture we used $\\mathcal{L}(\\mathbf{w}) = \\sum_{i\\in S} \\log(1+e^{-z_i})$, omitting the division by $|S|$. Since we generally assume that the number of examples $|S|$ is fixed, these two forms of the loss differ by a constant factor, so they are pretty much equivalent.\n",
    "\n",
    "Alternatively, suppose we use a different convention, that negative examples are labeled $0$ instead. That is, $y_i\\in\\{0,1\\}$. Just as in the previous case, we predict $+1$ with probability $\\hat{y}_i = 1/(1+e^{-\\mathbf{w} \\cdot \\mathbf{x_i}})$. Thus, we predict $0$ with probability $1 - \\hat{y}_i = 1 - 1/(1+e^{-\\mathbf{w} \\cdot \\mathbf{x_i}}) = 1/(1+e^{\\mathbf{w} \\cdot \\mathbf{x_i}})$.\n",
    "\n",
    "Then, observe that the probability of predicting the right label is equal to $\\hat{y}_i$ when $y_i = 1$, and is equal to $1 - \\hat{y}_i$ when $y_i = 0$. Then, do you see why the loss can be expressed as the following? (You can plug in $y_i = 1$ or $y_i = 0$.)\n",
    "$$\\mathcal{L}(\\mathbf{w}) =\n",
    "  -\\frac{1}{|S|} \\sum_{i\\in S} \\big(y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)\\big)\n",
    "~~ \\mbox{ where } ~~ \n",
    "\\hat{y}_i = \\frac{1}{1+e^{-\\mathbf{w} \\cdot \\mathbf{x_i}}}\n",
    "~~.$$\n",
    "\n",
    "If we find the gradient for the first form (when $y_i\\in\\{-1,+1\\}$), we obtain the following:\n",
    "$$ \\nabla_{\\mathbf{w}} \\mathcal{L} = -\\frac{1}{|S|} \\sum_{i\\in S} q_i y_i \\mathbf{x}_i \n",
    "~~ \\mbox{ where } ~~ q_i = \\frac{1}{1 + e^{z_i}} ~~ .\n",
    "$$\n",
    "\n",
    "For the second form (when $y_i\\in\\{0,+1\\}$), we find that the gradient is:\n",
    "$$ \\nabla_{\\mathbf{w}} \\mathcal{L} = \\frac{1}{|S|} \\sum_{i\\in S} \\big(\\hat{y}_i - y_i\\big) \\mathbf{x}_i  ~~ ,$$\n",
    "where $\\hat{y}_i$ is as defined above. You can verify these gradient values for yourself.\n",
    "\n",
    "For your own practice (ungraded), you should confirm the equivalence of these two forms, and derive them for yourself. That is, calculate both gradients (see if you get the same thing we did). Then, verify that when you plug in $y_i = 1$ you get the same thing, and when you plug in $y_i = -1$ or $=0$ respectively, you get the same thing. If you're having trouble with this, please ask on Piazza!\n",
    "\n",
    "Finally, implement the following functions for the case where $y_i\\in\\{-1,+1\\}$. You will use these functions throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate loss from margins\n",
    "# Here, z is equal to y times X dot w\n",
    "# That is, z_i = y_i * x_i dot w\n",
    "def logloss_from_z(z):\n",
    "    ### ACT 1\n",
    "    #l = [np.log(1 + np.exp(-z_i)) for z_i in z]\n",
    "    #return np.average(l)\n",
    "    S = len(z)\n",
    "    l = np.zeros(S)\n",
    "    for i in range(S):\n",
    "        l[i] = np.log(1+(np.exp(-z[i])))\n",
    "        \n",
    "    L =(1/S)*(np.sum(l))\n",
    "    return L\n",
    "    \n",
    "\n",
    "# Calculate error from margins\n",
    "# the error is the proportion of examples that you \"get wrong\".\n",
    "# a.k.a. the examples where you assign less than 50% chance to the correct label\n",
    "# This is equivalent to the proportion of examples where z_i is less than 0\n",
    "def error_from_z(z):\n",
    "    ### ACT 2\n",
    "    count = np.sum(np.array(z) < 0, axis=0)\n",
    "    return count[0]/len(z)\n",
    "    \n",
    "\n",
    "# Calculate loss from parameter vector\n",
    "def logloss(X, y, w):\n",
    "    ### ACT 3\n",
    "    #z = y*np.dot(X,w)\n",
    "    z = np.multiply(y,np.dot(X,w))\n",
    "    return logloss_from_z(z)\n",
    "    \n",
    "\n",
    "# Calculate error from parameter vector\n",
    "def error(X, y, w):\n",
    "    ### ACT 4\n",
    "    #z = y*np.dot(X,w)\n",
    "    z = np.multiply(y,np.dot(X,w))\n",
    "    return error_from_z(z)\n",
    "    \n",
    "\n",
    "# Gradient of LogLoss w.r.t. w\n",
    "def logloss_gradient(X, y, w):\n",
    "    ### ACT 5\n",
    "    z = y*np.dot(X,w)\n",
    "    q = [1/(1+np.exp(z_i)) for z_i in z] #vector of q's\n",
    "\n",
    "    # column-wise multiplication of example matrix with the vector of constants\n",
    "    # i.e. each row = (q_i * y_i) * X_row_i\n",
    "    # results in a nxd matrix\n",
    "    L_grad = (X.T * (q*y)).T\n",
    "    #print(L_grad)\n",
    "    \n",
    "    L_grad = np.sum(L_grad,axis=0) # sum across the columns to find total gradient in each parameter\n",
    "    L_grad_normed = -L_grad/len(y) # normalize, divide each column sum by -1/sample_size\n",
    "    \n",
    "    #print(L_grad)\n",
    "    #print(L_grad_normed)\n",
    "    return L_grad_normed\n",
    "    \n",
    "\n",
    "# Calculate error & loss from parameter vector\n",
    "# Return two floats in the format (loss, error)\n",
    "def logloss_and_error(X, y, w):\n",
    "    ### ACT 6\n",
    "    l = logloss(X, y, w)\n",
    "    e = error(X,y,w)\n",
    "    return l,e\n",
    "    \n",
    "\n",
    "#ASSERTS\n",
    "assert(round(logloss_from_z(np.array([0,2,4]).reshape(-1,1)),15) == 0.2794083731735760)\n",
    "assert(error_from_z(np.array([0,-2,4,1]).reshape(-1,1)) == 1/4)\n",
    "\n",
    "X = np.array([[1, 2, 3], [3, 4, 3], [1, 3, 5]])\n",
    "y = np.array([1, -1, 1]).reshape(-1,1)\n",
    "w = np.array([0.1, 0.2, 0.1]).reshape(-1,1)\n",
    "\n",
    "assert(round(logloss(X,y,w),15) == 0.7516001810680870)\n",
    "assert(error(X,y,w) == 1/3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "Normally, we initialize $w$ to be zero (as we discussed in the last assignment). This means we initially predict everything as 0.5 (a.k.a. 50-50). This might not make sense. For example, in the MNIST problem, we are distinguishing a single digit from the 9 other digits. On average, there's only a 10% chance each image is our desired digit, so initially maybe we should predict something lower.\n",
    "\n",
    "This is where bias comes in. Bias is something that is added to every prediction. For example, if we have bias $b$, an example $x_i$, and a weight vector $\\mathbf{w}$, we would predict the following probability:\n",
    "$$\\Pr[y_i = +1] = \\frac{1}{1 + e^{-(\\mathbf{w}\\cdot \\mathbf{x}_i + b)}}$$\n",
    "\n",
    "So, in MNIST, we'd want to initialize bias to be negative, since initially we would predict that an average digit should have less than 50% chance to be classified as $+1$.\n",
    "\n",
    "Bias is actually easy to add into our models.\n",
    "\n",
    "Rather than creating a new variable $b$, we simply append $1$ to each example $\\mathbf{x}_i$. (That is, we append a column of ones to the data matrix $X$.) Since the length of $\\mathbf{x}_i$ has increased by one, the length of our weight vector $\\mathbf{w}$ has also increased by one. Let's call the final entry of our weight vector $w_d$, and call the final entry of our example $x_{i, d}$ (so $x_{i, d} = 1$). Then, when we compute $\\mathbf{w}\\cdot \\mathbf{x}_i$, we get\n",
    "$$\\mathbf{w} \\cdot \\mathbf{x}_i = w_1x_{i, 1} + w_2x_{i, 2} + \\ldots + w_dx_{i, d}$$\n",
    "which equals the previous dot product, plus $w_dx_{i, d}$ which equals $w_d$.\n",
    "\n",
    "Thus, $w_{d}$ becomes the bias term: it is added to every dot product regardless of the example $\\mathbf{x}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Initial Bias Using Log-Odds (ACT's 7-9)\n",
    "As mentioned above, we don't necessarily want to initialize the bias term of $w$, $w_{d}$, to zero. Below is a small math exercise where you will find the best value to initialize bias.\n",
    "\n",
    "Total number of positive examples is $n_+$ (pos) and negative $n_-$ (neg). Assume $\\mathbf{w}=\\mathbf{0}$ except for the weight of the last feature $w_{d}$. Since we have the other entries of $w$ initialized equal to $0$, we can write the logistic loss as the following:\n",
    "$$\\frac1n \\sum_{i:y_i=+1} \\log(1+\\exp(-w_d)) + \\frac1n \\sum_{i:y_i=-1} \\log(1+\\exp(w_d))$$\n",
    "If you're unclear on how we got this, verify it by plugging in $\\mathbf{w}$ into the formula for logistic loss. Most of the terms in the dot product equal zero except for the last term in the dot product. For reference: $\\exp(-w_d) = e^{-w_d}$.\n",
    "\n",
    "To find the best initialization for $w_d$, we take the derivative of the above loss, and set it to $0$. The ACT's below ask you to do this, to find the best initial bias. Submit your answer as a LaTeX'd answer within this notebook cell (or make a new notebook cell).\n",
    "\n",
    "ACT 7: Take the derivative, with respect to $w_{d}$, of the above expression for logistic loss.\n",
    "\n",
    "ACT 8: Set the derivative of this to zero, and solve for w_d.\n",
    "\n",
    "ACT 9: Fill in the value of $w_d$ below which minimizes your initial loss, then code it below. (We've given you a hint, the answer is a log of something.)\n",
    "$$\n",
    "w_d = \\log\\left( \\mathbf{\\mbox{ACT 9}} \\right) ~.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACT 7:\n",
    "\n",
    "$$ \\frac{dy}{dw_d} = \\frac{n_+}{n}\\cdot\\frac{-e^{-w_d}}{1+e^{-w_d}} + \\frac{n_-}{n}\\cdot\\frac{e^{w_d}}{1+e^{w_d}} $$\n",
    "\n",
    "where:\n",
    "\n",
    "$n_{+}$ = number of positive labels\n",
    "\n",
    "$n_{-}$ = number of negative labels\n",
    "\n",
    "\n",
    "### simplify to:\n",
    "\n",
    "$$ \\frac{dy}{dw_d} = \\frac{-n_{+} + n_{-}\\cdot e^{w_d}}{n(1+e^{w_d})} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACT 8:\n",
    "\n",
    "$$ \\frac{dy}{dw_d} = \\frac{-n_{+} + n_{-}\\cdot e^{w_d}}{n(1+e^{w_d})} = 0$$\n",
    "\n",
    "$$ e^{w_d} = \\frac{n_{+}}{n_{-}}$$\n",
    "\n",
    "$$ w_d = log\\left(\\frac{n_{+}}{n_{-}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the initial bias\n",
    "def init_bias(pos, neg):\n",
    "    ### ACT 9 continued\n",
    "    return pos/neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image normalization and flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working with MNIST, we need to normalize and flatten the input images. After all, we do not normally perform logistic regression on 2D input data like images, we normally perform logistic regression on 1D data! So, we first flatten the image so it becomes 1-dimensional.\n",
    "\n",
    "First we flatten the data set of images: each $p_x \\times p_y$ image becomes a 1d vector of size $d = p_x \\, p_y$.\n",
    "\n",
    "This amounts to reshaping $X$ from a $n \\times p_x \\times p_y$ tensor to a $n \\times d$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\na = flatten_images(X)\\nassert(a.shape[0] == X.shape[0])\\nassert(a.shape[1] == X.shape[1]*X.shape[2])\\nassert(a[0][5] == 4)\\nassert(a[1][13] == 2)\\nassert(a[2][9] == 7)\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_images(X):\n",
    "    ### ACT 10\n",
    "    X_reshaped = np.zeros((X.shape[0],X.shape[1]*X.shape[2])) #60,000x784\n",
    "    \n",
    "    for i,image in enumerate(X):\n",
    "        #image_vec = image.ravel()\n",
    "        image_vec = np.ravel(image.T)\n",
    "        X_reshaped[i,:] = image_vec.reshape(1,-1)\n",
    "        \n",
    "    return(X_reshaped)\n",
    "    \n",
    "#ASSERTS\n",
    "X = np.zeros((3,4,4))\n",
    "X[0][1][1] = 4\n",
    "X[0][3][3] = 7\n",
    "\n",
    "X[1][0][1:3] = 3\n",
    "X[1][3][1] = 2\n",
    "\n",
    "X[2][2][1:5] = 7\n",
    "X[2][3][1] = 2\n",
    "\n",
    "\"\"\"\n",
    "a = flatten_images(X)\n",
    "assert(a.shape[0] == X.shape[0])\n",
    "assert(a.shape[1] == X.shape[1]*X.shape[2])\n",
    "assert(a[0][5] == 4)\n",
    "assert(a[1][13] == 2)\n",
    "assert(a[2][9] == 7)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126\n",
      "   136 175  26 166 255 247 127   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253\n",
      "   253 225 172 253 242 195  64   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253\n",
      "   251  93  82  82  56  39   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247\n",
      "   241   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43\n",
      "   154   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108\n",
      "     1   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253\n",
      "   119  25   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253\n",
      "   253 150  27   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93\n",
      "   252 253 187   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   249 253 249  64   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183\n",
      "   253 253 207   2   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253\n",
      "   253 250 182   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253\n",
      "   201  78   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81\n",
      "     2   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]\n",
      "  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0]]]\n",
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.  55. 136.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0. 172. 253.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.  18. 226. 253.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.  49.  18.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0. 171. 253. 253.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.  30. 238. 219.  80.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.  23. 219. 253. 212.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.  36. 253. 253. 156.  14.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.  66. 253. 253. 135.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.  94. 253. 253. 107.   1.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.  24. 213. 253. 253. 132.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0. 154. 253. 253. 253. 154. 139.  11.   0.\n",
      "    0.   0.   0.   0.   0.   0. 114. 253. 253. 244.  16.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   3. 170. 253. 253. 253. 253. 253. 190.  35.\n",
      "    0.   0.   0.   0.   0.  39. 221. 253. 253. 133.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.  18. 253. 253. 253. 205.  90. 190. 253. 241.\n",
      "   81.   0.   0.   0.   0. 148. 253. 253. 195.  11.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.  18. 253. 253. 198.  11.   0.   2.  70. 225.\n",
      "  240.  45.   0.   0.  46. 229. 253. 253.  80.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.  18. 253. 253. 182.   0.   0.   0.   0. 160.\n",
      "  253. 186.  16.   0. 130. 253. 253. 198.   9.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0. 126. 253. 253. 247.  43.   0.   0.   0. 108.\n",
      "  253. 253.  93.   0. 183. 253. 253.  81.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0. 136. 253. 251. 241. 154.   0.   0.   0.   1.\n",
      "  119. 253. 252. 249. 253. 253. 201.   2.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0. 175. 225.  93.   0.   0.   0.   0.   0.   0.\n",
      "   25. 150. 253. 253. 253. 250.  78.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.  26. 172.  82.   0.   0.   0.   0.   0.   0.\n",
      "    0.  27. 187. 249. 207. 182.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0. 166. 253.  82.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.  64.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0. 255. 242.  56.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0. 247. 195.  39.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0. 127.  64.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:1])\n",
    "print(flatten_images(X_train[0:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to normalize each image. This is useful because some images may be overall darker or lighter, or higher- or lower-contrast than others. We don't want this to affect our classification. Thus, we normalize each image by making sure each image's average \"brightness\" (a.k.a. pixel value) is 0, and each image's pixels have standard deviation one.\n",
    "\n",
    "Finally, we want to provide the option to append a bias column of ones to the data matrix.\n",
    "\n",
    "In your method below, normalize each image, by subtracting the average value of that image over all its pixels, and\n",
    "dividing by the standard deviation of the image's pixel values. If the bias argument is non-zero, add a bias term by appending `bias` to each example. For example, if `bias=1`, then append an entry of $1$ to each example. Algebraicly, flattening and normalizing amounts to flattening each image to a $d$ dimensional vector\n",
    "$\\mathbf{x}$ and calculating average pixel value and variance of pixel values,\n",
    "$$ m = \\mathbb{E}(\\mathbf{x}) = \\frac 1 d \\sum_{i=1}^d x_i ~~~\n",
    "   s^2 = \\mathbb{V}(\\mathbf{x}) = \\frac 1 d \\sum_{i=1}^d x_i^2 - m^2 ~~ . $$\n",
    "Here, $m$ is the image's mean and $s$ is the image's standard deviation. (You can use pre-existing numpy functions to compute these.)\n",
    "\n",
    "Define $a=s^{-1}$, then the normalized image (represented as a vector) is, ${a (\\mathbf{x} - m)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each example as described above. If bias is non-zero, add a bias term by appending `bias` to\n",
    "# each example\n",
    "def normalize(X, bias=0):\n",
    "    ### ACT 11\n",
    "    m = X.mean(1).reshape(-1,1)\n",
    "    s = np.std(X, axis=1).reshape(-1,1)\n",
    "    \n",
    "    a = 1/s\n",
    "    \n",
    "    X_normed = (X - m)\n",
    "    \n",
    "    X_normed = a*X_normed\n",
    "    \n",
    "    if bias:\n",
    "        #Add column of 'bias' at the end\n",
    "        col = np.arange(X_normed.shape[0])\n",
    "        col = np.full_like(col,bias).reshape(-1,1)\n",
    "        X_normed = np.append(X_normed, col, axis=1)\n",
    "    \n",
    "    return X_normed\n",
    "\n",
    "def normalize2(X, bias=0):\n",
    "    ### ACT 11\n",
    "    \n",
    "    if bias:\n",
    "        #Add column of 'bias' at the end\n",
    "        col = np.arange(X.shape[0])\n",
    "        col = np.full_like(col,bias).reshape(-1,1)\n",
    "        X = np.append(X, col, axis=1)\n",
    "        \n",
    "    m = X.mean(1).reshape(-1,1)\n",
    "    s = np.std(X, axis=1).reshape(-1,1)\n",
    "    \n",
    "    a = 1/s\n",
    "    \n",
    "    X_normed = (X - m)\n",
    "    \n",
    "    X_normed = a*X_normed\n",
    "\n",
    "    \n",
    "    return X_normed\n",
    "\n",
    "#ASSERTS\n",
    "X = np.zeros((3,4,4))\n",
    "X[0][1][1] = 4\n",
    "X[0][3][3] = 7\n",
    "\n",
    "X[1][0][1:3] = 3\n",
    "X[1][3][1] = 2\n",
    "\n",
    "X[2][2][1:5] = 7\n",
    "X[2][3][1] = 2\n",
    "\n",
    "a = flatten_images(X)\n",
    "\n",
    "#Without Bias\n",
    "a_normed = normalize(a,0)\n",
    "assert(a_normed.mean(1).reshape(-1,1).sum() < 0.0000000000001) #Check mean ~0\n",
    "assert(a_normed.shape[1] == a.shape[1]) #Check correct number of columns\n",
    "\n",
    "#With Bias\n",
    "bias = 2\n",
    "a_normed = normalize(a,bias)\n",
    "assert(a_normed[0][-1] == bias) #Check last number = bias\n",
    "assert(a_normed.shape[1] == a.shape[1]+1) #Check that added column\n",
    "\n",
    "#print(X_train[2:3])\n",
    "#a = normalize(flatten_images(X_train[2:3])).reshape(28,28)\n",
    "#a = np.round(a,2)\n",
    "#print(a)\n",
    "\n",
    "#print(y_train[0:20])\n",
    "#print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and flatten the training and test images\n",
    "# You may print Xtr and Xte, or view their shape, to get a feeling for their structure\n",
    "Xtr = normalize(flatten_images(X_train), bias=1)\n",
    "Xte = normalize(flatten_images(X_test), bias=1)\n",
    "\n",
    "#print (Xtr[0])\n",
    "\n",
    "#Xtr = normalize(flatten_images(X_train), bias=1)\n",
    "#print (Xtr[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the train labels $\\mathbf{y}^{\\mbox{tr}}$ and test labels $\\mathbf{y}^{\\mbox{te}}$ to vectors over\n",
    "$\\{-1,+1\\}$ indicating whether each example is an 8.\n",
    "\n",
    "For example, `ytr8[i] = -1` if the i-th example is NOT an 8\n",
    "\n",
    "Similarly to above, viewing the first few entries of ytr8,\n",
    "may give you some intuition about their contents and structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr8 = (2 * (y_train == 8) - 1).reshape(len(y_train), 1)\n",
    "yte8 = (2 * (y_test == 8) - 1).reshape(len(y_test), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACT 12: For coordinate descent use the mean value theorem to form a quadratic upper-bound on the loss for a single coordinate $j$. When using the convention of $y_i\\in\\{0,1\\}$ show that,\n",
    "    $$\n",
    "     |S|\\cdot\\mathcal{L}(\\mathbf{w}+\\delta\\mathbf{1}_j) \\leq\n",
    "       \\kappa + \\sum_i (\\hat{y}_i - y_i) X_{ij} \\, \\delta + \\frac18 \\sum_i  X_{ij}^2 \\, \\delta^2 ~~.\n",
    "    $$\n",
    "Here we use $\\mathbf{w}+\\delta\\mathbf{1}_j$ to denote $\\mathbf{w}$ plus $\\delta$ in the $j$-th coordinate. Hint: The slides titled \"Mean Value Theorem\" and \"Putting It All Together\" in the Logistic Regression lecture should be helpful. Please ask on Piazza if you're having trouble understanding ACT 12 or the relevant lecture slides!\n",
    "\n",
    "Note: The factor of $|S|$ multiplied on the left side is used to make this equation similar to the one proved in lecture. In this notebook we divided error and loss by $|S|$ to get average error or loss. However, in the logistic regression lecture we tried to minimize total loss (a.k.a. we didn't divide loss by $|S|$). Minimizing these two losses _is equivalent_ since $|S|$ is just a constant factor, so we multiply by $|S|$ on the left to keep things consistent with lecture in this ACT.\n",
    "\n",
    "ACT 13: Denote the $j^{\\mbox{th}}$ column of $X$ by $\\mathbf{v}_j$ and let $c_j= \\|\\mathbf{v}_j\\|^2$.\n",
    "\n",
    "Show that the $\\delta^\\star$ minimizing the above bound $\\kappa + \\sum_i (\\hat{y}_i - y_i) X_{ij} \\, \\delta + \\frac18 \\sum_i  X_{ij}^2 \\, \\delta^2$ is,\n",
    "$$\n",
    "\\delta^\\star = \\frac{4}{c_j} (\\mathbf{y - \\hat{y}) \\cdot v_j} = \\frac{4}{c_j} \\sum_{i=1}^n (y_i - \\hat{y}_i)\\mathbf{v}_j[i] ~~.\n",
    "$$\n",
    "\n",
    "(Note that $\\mathbf{v}_j[i] = X_{ij}$.) Hint: Take the derivative and set it equal to zero.\n",
    "\n",
    "ACT 14: Of course the above expression assumes that $y_i\\in \\{0, 1\\}$, which is different from our current setting, where $y_i \\in \\{-1, 1\\}$.\n",
    "\n",
    "To convert back to the $\\{-1, 1\\}$ setting, consider the positive and negative examples in the above summation. Positive examples have $y_i = 1$, so that term in the summation becomes $(1 - \\hat{y}_i)\\mathbf{v}_j[i]$. Negative examples have $y_i = 0$, so that term in the summation becomes $(-\\hat{y}_i)\\mathbf{v}_j[i]$.\n",
    "\n",
    "Can you rewrite the expression for $\\delta^\\star$ when $y_i\\in\\{-1, 1\\}$?\n",
    "\n",
    "Hint: First, re-write $(y_i - \\hat{y}_i)$ from the previous setting in terms of the new $y_i$ and $z_i$, or $y_i$ and $q_i$. You should get a simple expression that replaces $(y_i - \\hat{y}_i)$. The other terms in the expression for $\\delta^\\star$ shouldn't change, since they don't depend on $y_i$. Thus, you can just plug in your new expression for $(y_i - \\hat{y}_i)$ into the previous expression for $\\delta^\\star$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACT 12\n",
    "\n",
    "recall:\n",
    "\n",
    "$$ c_i = -(\\mathbf{w} \\cdot \\mathbf{X_{i}} + \\delta_j\\cdot X_{ij})$$\n",
    "$$ \\hat{b}_i = \\frac{1}{1 + e^{c_i}} $$\n",
    "$$ \\hat{y}_i = \\frac{1}{1 + e^{-\\mathbf{w}\\cdot \\mathbf{x_i}}} $$ \n",
    "\n",
    "Use MVT with upper bounding to arrive at the shown equation. MVT can be expressed as the following:\n",
    "\n",
    "$$ L(\\mathbf{w} + \\delta_j) = L(\\mathbf{w} + 0) + \\delta_j \\cdot L'(\\mathbf{w} + 0) + \\frac{\\delta_{j}^2}{2} \\cdot L''(\\mathbf{w} + \\alpha)$$\n",
    "\n",
    "Find first derivative, $L'$:\n",
    "\n",
    "$$ \\frac{dL(\\mathbf{w} + \\delta_j)}{d\\delta_j} = \\frac{dL(\\mathbf{w} + \\delta_j)}{d\\hat{b}} \\cdot \\frac{d\\hat{b}}{dc} \\cdot \\frac{dc}{\\delta_j} $$\n",
    "\n",
    "$$ \\frac{dL(\\mathbf{w} + \\delta_j)}{d\\delta_j} = \\sum_i \\left[\n",
    "    \\left(\\frac{\\hat{b}_i - y_i}{\\hat{b}_i(1 - \\hat{b}_i)}\\right)  \n",
    "    \\left(-\\hat{b}_i(1 - \\hat{b}_i)\\right)\n",
    "    \\left(-X_{ij}\\right)\n",
    "\\right]$$\n",
    "\n",
    "$$ \\frac{dL(\\mathbf{w} + \\delta_j)}{d\\delta_j} = \\sum_i(\\hat{b_i} - y_i)(X_{ij})$$\n",
    "\n",
    "when $\\delta = 0$ then $\\hat{b_i} = \\hat{y_i}$ so the first part of equation:\n",
    "\n",
    "$$ L'(\\mathbf{w} + 0) = \\sum_i(\\hat{y_i} - y_i)(X_{ij}) $$\n",
    "\n",
    "Find second derivative, $L''$:\n",
    "\n",
    "$$ \\frac{dL'(\\mathbf{w} + \\delta_j)}{d\\delta_j} = \\frac{dL'(\\mathbf{w} + \\delta_j)}{d\\hat{b}} \\cdot \\frac{d\\hat{b}}{dc} \\cdot \\frac{dc}{\\delta_j} $$\n",
    "\n",
    "\n",
    "$$ \\frac{dL'(\\mathbf{w} + \\delta_j)}{d\\delta_j} = \\sum_i \\left[\n",
    "    \\left(X_{ij}\\right)\n",
    "    \\left(-\\hat{b}_i(1 - \\hat{b}_i)\\right)\n",
    "    \\left(-X_{ij}\\right)\n",
    "\\right]$$\n",
    "\n",
    "$$ \\frac{dL'(\\mathbf{w} + \\delta_j)}{d\\delta_j} = \\sum_i X^{2}_{ij} \\hat{b}_i(1 - \\hat{b}_i) $$\n",
    "\n",
    "Bound the 2nd derivative using the the fact that $ \\hat{b}_i(1 - \\hat{b}_i) $ is a complimentary function with a maximum value of $\\frac{1}{4}$ with $\\hat{b}_i = 0.5$\n",
    "\n",
    "Putting it all together, remembering to calculate the total error so multiply LHS by $|S|$:\n",
    "\n",
    "$$ |S|  L(\\mathbf{w} + \\delta_j) \\leq L(\\mathbf{w} + 0) + \\delta_j \\sum_i(\\hat{y_i} - y_i)(X_{ij}) +\\frac{\\delta_{j}^2}{8} \\sum_i X^{2}_{ij}$$\n",
    "\n",
    "This derivation matches the one presented above, note that some of the terms have been adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACT 13\n",
    "\n",
    "expand the function as:\n",
    "\n",
    "$$ |S|L(\\mathbf{w} + \\delta_j) \\leq L(\\mathbf{w} + 0) + \\delta_j \\sum_i(\\hat{y_i})(X_{ij}) - \\delta_j \\sum_i(y_i)(X_{ij}) +\\frac{\\delta_{j}^2}{8} \\sum_i X^{2}_{ij}$$\n",
    "\n",
    "now the jth column of X is $v_j$, realize that $c_j = ||v_j||^2 = \\sum_iX_{ij}^2$\n",
    "\n",
    "doing a summation over every element of two columns is the same as dot product, therefore $\\sum\\hat{y_i}X_{ij} = \\hat{\\mathbf{y}} \\cdot \\mathbf{v_j}$\n",
    "\n",
    "$$ |S|L(\\mathbf{w} + \\delta_j) \\leq L(\\mathbf{w} + 0) + \\delta_j (\\hat{\\mathbf{y}}\\cdot \\mathbf{v_j}) - \\delta_j (\\mathbf{y}\\cdot\\mathbf{v_j}) +\\frac{\\delta_{j}^2}{8} c_j$$\n",
    "\n",
    "Take the derivative of this function w.r.t. $\\delta_j$ and set it to 0:\n",
    "\n",
    "$$ (\\hat{\\mathbf{y}}\\cdot\\mathbf{v_j}) - (\\mathbf{y}\\cdot\\mathbf{v_j}) + \\frac{\\delta_{j}}{4} c_j = 0$$\n",
    "\n",
    "Solving this equation leads to:\n",
    "\n",
    "$$\\delta_{j}^* = \\frac{4}{c_j}(\\mathbf{y} - \\hat{\\mathbf{y}}) \\cdot \\mathbf{v_j} = \\frac{4}{c_j} \\sum_{i=1}^n (y_i - \\hat{y}_i)\\mathbf{v}_j[i]   $$\n",
    "\n",
    "This is equal to the value that is presented in the question. This can be re-written in summation form as well, which results in the same value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACT 14\n",
    "\n",
    "Note that $ X_{ij}$ wil replace $ {v}_j[i]$ for clarity\n",
    "\n",
    "By splitting the resulting $\\delta^*$ value into the 0/1 summation we get:\n",
    "\n",
    "$$ \\delta^* = \\frac{4}{c_j} \\left[ \\sum_{i:y=+1}^n (1-\\hat{y_i})X_{ij} + \\sum_{i:y=0}^n(-\\hat{y}_i)X_{ij} \\right]$$\n",
    "\n",
    "The left summation corresponds to the cases where y = 1, and the right corresponds to cases where y = 0. To transfer this function into -1/1 only change the values of y, the rest of the terms will not change.\n",
    "\n",
    "remember that $\\hat{y} = \\frac{1}{1 + e^{-wx}}$ and $q = \\frac{1}{1 + e^{ywx}}$\n",
    "\n",
    "Looking at the $y = +1$ --> $y = +1$  case:\n",
    "\n",
    "$$ (1 - \\hat{y}) = \\frac{1}{1+e^{wx}} $$\n",
    "\n",
    "which is equivalent to the following when $y = +1$: \n",
    "\n",
    "$$ \\frac{1}{1+e^{wx}} = \\frac{y}{1+e^{ywx}} = y \\cdot q$$\n",
    "\n",
    "Looking at the $y = 0$ ---> $y = -1$ case:\n",
    "\n",
    "$$ (-\\hat{y}) = \\frac{-1}{1+e^{-wx}} $$\n",
    "\n",
    "which is equivalent to the following when $y = -1$: \n",
    "\n",
    "$$ \\frac{-1}{1+e^{-wx}} = \\frac{y}{1+e^{ywx}} = y \\cdot q$$\n",
    "\n",
    "So basically the transformation from 0/1 ---> -1/1 is the following:\n",
    "\n",
    "$$ \\delta^* = \\frac{4}{c_j} \\sum_{i=1}^n (y_i - \\hat{y}_i)X_{ij} --> \\frac{4}{c_j} \\sum_{i=1}^n (y_i q_i)X_{ij}$$\n",
    "\n",
    "Now we can break it up into +1/-1 summations:\n",
    "\n",
    "$$ \\delta^* = \\frac{4}{c_j} \\left[ \\sum_{i:y=+1}^n q_i X_{ij} - \\sum_{i:y=-1}^n q_i X_{ij} \\right] $$\n",
    "\n",
    "where the summations are $n_+$ and $n_-$ respectively:\n",
    "\n",
    "$$ \\delta^* = \\frac{4}{c_j} \\left[ n_+ - n_- \\right] $$\n",
    "\n",
    "This is the same as the derivation that was done in class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "# `x` is the j-th column of X (a.k.a. the v_j from above)\n",
    "# `y` is the vector of labels\n",
    "# `z` is a vector with entries z_i\n",
    "# `cj` is c_j from above, the squared norm of `x`\n",
    "def delta_wj(x, y, z, cj):\n",
    "    ### ACT 15 (code your result from ACT 14 here)\n",
    "\n",
    "    q = list(map(lambda x:1/(1 + np.exp(x)),z.T[0]))\n",
    "    q = np.asarray(q)\n",
    "    \n",
    "    label_split = (y==1).T[0]\n",
    "    \n",
    "    n_plus = np.sum(x[label_split] * q[label_split])\n",
    "    n_minus = np.sum(x[~label_split] * q[~label_split])\n",
    "    \n",
    "    return (4/cj)*(n_plus - n_minus)\n",
    "    \n",
    "#ASSERT\n",
    "x = np.array([1,-1,2,-2,1])\n",
    "y = np.array([1,-1,-1,1,1]).reshape(-1,1)\n",
    "z = np.array([10,20,10,-10,10]).reshape(-1,1) #One here incorrect\n",
    "cj = 10\n",
    "\n",
    "assert(np.isclose(delta_wj(x, y, z, cj),-0.8,atol=1e-04) == True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "from numpy.random import permutation\n",
    "\n",
    "# You don't need to use this class yourself: we provided the index-sampling code for you\n",
    "# sample new index (with or without replacement)\n",
    "# d is the max index\n",
    "class IndexSampler:\n",
    "    def __init__(self, d):\n",
    "        self.d = d\n",
    "        self.prm = None\n",
    "    \n",
    "    def sample_new_index(self, replace = 1):\n",
    "        if replace:\n",
    "            return randint(self.d)\n",
    "        if self.prm is None:\n",
    "            self.prm = permutation(self.d)\n",
    "            self.head = 0\n",
    "        ind = self.prm[self.head]\n",
    "        self.head += 1\n",
    "        if self.head == self.d:\n",
    "            self.head = 0\n",
    "            self.prm = None\n",
    "        return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs is maximum number of epochs to train\n",
    "# eps is your termination condition number, similar to in PA2's linear regression\n",
    "# Every epoch, report the loss (we've provided some code that reports loss for you)\n",
    "# (An epoch consists of d updates)\n",
    "\n",
    "### ACT 16\n",
    "def logistic_regression_cd(X, y, epochs=100, eps=0.001):\n",
    "    pstr = 'Epoch: {0:2d}  Loss: {1:5.3f}  Error: {2:5.3f}'\n",
    "    n, d = X.shape\n",
    "    ### initialize w, z, c, errors, and losses\n",
    "    ### c is a vector whose j-th entry is equal to c_j from above\n",
    "    ### errors should be a vector containing the error at each step\n",
    "    ### losses should be a vector containing the loss at each step\n",
    "    ### NOTE: Don't forget to initialize the last entry of w as initial bias!\n",
    "    \n",
    "    # Initialize 'w'\n",
    "    w = np.zeros((d,1))\n",
    "    \n",
    "    neg = np.sum(y < 0, axis=0) # 5851 ~ 10%\n",
    "    pos = np.sum(y > 0, axis=0) # 54149 ~ 90%\n",
    "    \n",
    "    w[-1] = np.log(init_bias(pos,neg)) # bias = -2.22512692\n",
    "    \n",
    "    # Initialize 'z'\n",
    "    z = np.multiply(y,np.dot(X,w)) # all values are magnitude of bias (+ or -), since only bias term in w, all else is 0\n",
    "    \n",
    "    # Initialize 'c', sum of columns\n",
    "    c = np.sum(X**2, axis=0)\n",
    "    print(c)\n",
    "    \n",
    "    # Initialize 'losses' and 'errors'\n",
    "    # errors[0] = pos/(pos+neg), since initial guess is that there are no 8's, i.e. w*x_i = negative\n",
    "    losses = []\n",
    "    errors = []\n",
    "    \n",
    "    l,e = logloss_and_error(X,y,w)\n",
    "    \n",
    "    losses.append(l)\n",
    "    errors.append(e)\n",
    "    \n",
    "    print(\"\\n\\nInitialized Loss/Error\", l,e*100)\n",
    "    \n",
    "    cur_epoch = 0\n",
    "    sampler = IndexSampler(d)\n",
    "    for e in range(1, (d) * epochs + 1):\n",
    "        ### we've chosen a coordinate for you, now perform coordinate descent below\n",
    "\n",
    "        j = sampler.sample_new_index(replace = 0)\n",
    "        \n",
    "        w[j] += delta_wj(X[:,j],y,z,c[j])\n",
    "        z = np.multiply(y,np.dot(X,w))\n",
    "        \n",
    "        if e % (d) == 0:\n",
    "            ### update losses and errors with the current loss and error\n",
    "            cur_epoch += 1\n",
    "            \n",
    "            l = logloss_from_z(z)\n",
    "            e = error_from_z(z)\n",
    "                \n",
    "            losses.append(l)\n",
    "            errors.append(e)\n",
    "            \n",
    "            print(pstr.format(cur_epoch, losses[-1], errors[-1] * 100))\n",
    "            if (losses[-2] - losses[-1]) / losses[-1] < eps: break\n",
    "        \n",
    "    print('\\n')\n",
    "    return w, losses, errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   2.   4.   4.   1. ]\n",
      " [ 1.   1.5  4.   4.   2. ]\n",
      " [ 2.   2.5  3.   3.   1. ]\n",
      " [ 0.   0.   3.   1.   1. ]\n",
      " [-1.  -1.   3.   1.   1. ]]\n",
      "[[-1.03209369 -0.29488391  1.17953565  1.17953565 -1.03209369  1.        ]\n",
      " [-1.18585412 -0.79056942  1.18585412  1.18585412 -0.39528471  1.        ]\n",
      " [-0.40089186  0.26726124  0.93541435  0.93541435 -1.73719807  1.        ]\n",
      " [-0.91287093 -0.91287093  1.82574186  0.          0.          1.        ]\n",
      " [-1.06904497 -1.06904497  1.60356745  0.26726124  0.26726124  1.        ]]\n",
      "[[ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]]\n",
      "[ 7.  13.5 59.  43.   8. ]\n",
      "\n",
      "\n",
      "Initialized Loss/Error 0.7254845199027546 40.0\n",
      "Epoch:  1  Loss: 0.608  Error: 20.000\n",
      "Epoch:  2  Loss: 0.583  Error: 20.000\n",
      "Epoch:  3  Loss: 0.561  Error: 20.000\n",
      "Epoch:  4  Loss: 0.538  Error: 20.000\n",
      "Epoch:  5  Loss: 0.528  Error: 20.000\n",
      "Epoch:  6  Loss: 0.508  Error: 20.000\n",
      "Epoch:  7  Loss: 0.487  Error: 20.000\n",
      "Epoch:  8  Loss: 0.464  Error: 20.000\n",
      "Epoch:  9  Loss: 0.451  Error: 20.000\n",
      "Epoch: 10  Loss: 0.434  Error: 20.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "\n",
    "X_small = np.array([[1, 2, 4, 4, 1],[1, 1.5, 4, 4, 2], [2, 2.5, 3, 3, 1], [0, 0, 3, 1, 1],[-1, -1, 3, 1, 1]])\n",
    "print(X_small)\n",
    "\n",
    "X_small_normed = normalize(X_small,bias=1)\n",
    "print(X_small_normed)\n",
    "\n",
    "y_small = np.array([1,1,-1,-1,-1]).reshape(-1,1)\n",
    "print(y_small)\n",
    "\n",
    "[w, loss, err] = logistic_regression_cd(X_small, y_small, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 11029.8754604   11029.8754604   11029.8754604   11029.8754604\n",
      "  11029.8754604   11029.8754604   11029.8754604   11029.732435\n",
      "  11033.61132382  11038.90645576  11046.65312292  11035.94894771\n",
      "  11033.70709729  11029.94403869  11031.9115091   11029.74712972\n",
      "  11029.83537593  11029.8754604   11030.31743653  11029.77933047\n",
      "  11029.8754604   11029.78327243  11029.77929966  11029.8754604\n",
      "  11029.8754604   11029.8754604   11029.8754604   11029.8754604\n",
      "  11029.8754604   11029.8754604   11029.8754604   11029.8754604\n",
      "  11029.65670511  11029.8754604   11029.76782255  11041.62821332\n",
      "  11085.85991434  11127.6247476   11168.19936074  11137.55030635\n",
      "  11084.39684839  11048.4432577   11036.00423988  11037.50448659\n",
      "  11031.24277256  11031.83479566  11032.60024051  11034.36315208\n",
      "  11042.10809592  11036.97655882  11029.53498085  11029.8754604\n",
      "  11029.8754604   11029.8754604   11029.8754604   11029.8754604\n",
      "  11029.8754604   11029.8754604   11029.68907979  11030.64144921\n",
      "  11031.41320314  11030.54002656  11047.43625965  11150.28765922\n",
      "  11439.86498862  11649.69971939  11685.1405684   11568.84244683\n",
      "  11331.9130659   11193.90447029  11083.57818615  11062.12987199\n",
      "  11096.53170931  11147.59295804  11182.56738543  11318.76369583\n",
      "  11327.98841775  11267.22732014  11178.97994998  11059.28527469\n",
      "  11043.82804942  11029.39888706  11029.8754604   11029.8754604\n",
      "  11029.8754604   11029.8754604   11029.5692198   11030.1958112\n",
      "  11029.07642637  11066.56398054  11243.6840047   11634.41103034\n",
      "  12520.45808089  13285.71308136  13185.66230818  12568.37328387\n",
      "  12048.43199729  11694.73811101  11488.85616325  11539.85542074\n",
      "  11582.78816072  11851.43957657  12370.83434498  12933.06330068\n",
      "  13131.47620508  12647.28147882  11910.05851867  11329.80026276\n",
      "  11118.42453818  11045.2725211   11029.78532136  11029.8754604\n",
      "  11029.8754604   11029.73753797  11031.87329204  11029.19246418\n",
      "  11084.2771543   11271.14453826  12045.30513198  13372.54217352\n",
      "  15494.49318013  16633.90521379  16373.76467215  15323.00834978\n",
      "  14410.49112146  14095.81607188  13843.83185335  14043.0572617\n",
      "  14740.05053256  16309.42514708  18098.13205223  19336.31484619\n",
      "  19289.76977701  17004.69956871  14422.31640928  12551.91283265\n",
      "  11524.8093064   11199.55685085  11068.1312822   11030.04306799\n",
      "  11029.8754604   11029.61939504  11027.98152616  11070.82456926\n",
      "  11333.9282646   12288.45823654  14213.79794733  17315.66244693\n",
      "  20978.54643913  22894.72558787  22753.23757348  21565.46077432\n",
      "  21145.99558834  21691.49350085  23137.72231483  25158.47576663\n",
      "  26999.57232044  29299.16394428  31793.54287577  33629.82326775\n",
      "  32360.51584608  27871.15001373  21185.84481421  15702.1199464\n",
      "  12726.77892213  11597.97391409  11214.54969605  11048.5651672\n",
      "  11029.8754604   11034.22472333  11056.37153709  11200.52942503\n",
      "  11873.76083447  14468.1635343   18790.80853189  25006.71472619\n",
      "  31102.01660999  34901.44516038  35699.65613307  35117.47689638\n",
      "  36222.32481672  39229.01803322  42631.92532838  45164.12652211\n",
      "  46728.50693148  47266.95429462  49184.63706032  52093.27564927\n",
      "  52358.29962729  47029.72462268  35559.19184872  23305.8358987\n",
      "  15578.6992686   12780.10397839  11740.66511701  11076.79392574\n",
      "  11029.8754604   11053.38275354  11161.34814405  11529.55144042\n",
      "  13280.99457951  18402.62209467  27038.46366218  37519.19477854\n",
      "  47652.91846742  54668.90659588  56953.82599467  58872.0910818\n",
      "  63342.28736082  68516.74419055  72450.33961531  72686.17200694\n",
      "  69396.78687277  65356.5117664   65253.42626435  69641.32886141\n",
      "  74607.96695155  72523.79241333  58646.38870983  38211.31540082\n",
      "  21859.20537093  15271.44699738  12628.03802329  11158.59862158\n",
      "  11029.8754604   11082.07286475  11306.12923567  12136.95608661\n",
      "  15774.95138329  25227.91735809  39410.90396152  55302.90174976\n",
      "  71447.53663443  83153.87992947  89185.88332036  94742.08126031\n",
      " 102022.91454835 107996.89769832 110048.45388012 105506.22660797\n",
      "  95052.89582079  83863.94362528  79512.66254175  85713.62023213\n",
      "  96722.40282104 101159.60995487  89699.91832358  61869.28526558\n",
      "  32346.69766732  19926.63546671  14448.32302919  11188.31074764\n",
      "  11029.8754604   11083.48917876  11519.29166497  13379.52894127\n",
      "  20206.71669733  35536.71627343  56931.24311998  79129.298149\n",
      " 101950.51172544 118386.68647244 126464.84771684 132956.2221568\n",
      " 139307.476727   142762.48540759 142004.66082378 134006.97939469\n",
      " 117715.90023157 100856.39296054  94613.72318065 102929.56398356\n",
      " 118852.24718542 130025.62203712 124248.40186425  92294.52694707\n",
      "  48242.88349651  25967.14546259  16317.06712251  11307.06315444\n",
      "  11029.8754604   11170.10999852  12003.51742365  15201.0909664\n",
      "  26627.71751301  49340.60755725  78558.86358187 107082.59673608\n",
      " 135987.23755982 153743.22002587 156680.48317012 155173.87501806\n",
      " 155016.88479881 155084.29224633 152838.24969992 146317.69987649\n",
      " 131204.12036084 114849.72570545 111513.97645861 121907.67815948\n",
      " 141107.03885345 157228.63697451 158346.46779293 125216.2818589\n",
      "  66567.84282505  32914.26134766  18577.22197323  11496.94567502\n",
      "  11029.8754604   11210.00215277  12419.39249901  17529.29345153\n",
      "  34993.24665497  67552.87159128 104117.71025915 138503.35275354\n",
      " 167641.80445544 175980.18837139 164782.37781405 150223.00565483\n",
      " 145077.75220112 145962.83089261 150073.02076499 151991.40645458\n",
      " 144325.46159306 132584.1916267  129865.24243579 141175.07371821\n",
      " 162930.73201604 182404.72912982 187202.58302983 153833.28076579\n",
      "  81932.45215498  37818.38166134  20560.5569012   11690.58126119\n",
      "  11031.31865836  11231.59867284  13180.95611336  19945.15478215\n",
      "  46072.47661257  91438.12944059 135145.02879298 170593.44222427\n",
      " 191108.73177542 180677.30455116 150707.02857816 128871.70683531\n",
      " 126556.81916288 138706.51988628 157746.74218701 174978.81271763\n",
      " 175809.20089389 162009.64244858 152538.04827708 163422.37959668\n",
      " 186154.1278425  205675.67823428 207263.4517869  169866.10468579\n",
      "  90144.01923518  40599.26579486  21428.67347007  11857.3326715\n",
      "  11046.92741136  11238.78238484  14015.07234991  23067.2090709\n",
      "  59405.57944891 118325.35069615 168567.1735017  201369.03047777\n",
      " 207581.24290441 178304.13951654 140643.2064767  125160.16852574\n",
      " 137590.89190378 173502.24942225 214201.76930504 236855.63590456\n",
      " 227919.72104306 201384.53811858 185765.19319198 190727.87689327\n",
      " 210356.33726299 223613.00639641 215501.06123893 171449.31450196\n",
      "  90112.34001663  40492.12503317  21225.15372979  11942.20675757\n",
      "  11036.21986341  11262.58324857  15010.46296437  26270.53762753\n",
      "  70583.26346936 139492.13113321 195501.6078605  224427.52362466\n",
      " 217471.45707486 181753.17950969 151819.21273589 153838.33448121\n",
      " 185510.56893704 231273.58307532 267297.84564683 276013.91585468\n",
      " 255010.17481214 223887.22681524 206760.24717882 208377.91168834\n",
      " 221667.5903888  226637.56617452 208743.39394956 157474.06586408\n",
      "  81233.40299818  36667.91228226  20163.89839873  12141.50669442\n",
      "  11029.79249707  11278.85112264  15604.06110437  28503.726562\n",
      "  75780.63003406 149077.5209411  208320.14681224 233738.64411842\n",
      " 221434.22619382 188698.2382192  168502.26394105 174248.46500377\n",
      " 202608.47705952 237196.61788672 260189.29965661 256312.16930489\n",
      " 235509.90130484 212466.58566718 199816.86019866 202692.73034305\n",
      " 211336.56542165 207239.55016626 180804.00573894 129063.46398477\n",
      "  66018.74452053  32082.87014699  18142.77542663  11975.20053542\n",
      "  11029.8754604   11309.15312592  15897.44851566  28037.22187625\n",
      "  73623.41419771 144479.466522   201667.8206421  227366.68474404\n",
      " 219011.24927303 192396.55750069 175036.05113893 176758.99368516\n",
      " 197894.75364287 222911.05476923 235903.64527751 229208.43703711\n",
      " 210738.78710941 190433.34693615 181590.89905352 184234.04002692\n",
      " 186682.11275039 173178.10657507 141020.98962171  95182.13534949\n",
      "  51020.52903777  27684.34226577  16712.0614563   11742.60395159\n",
      "  11029.8754604   11238.86151367  15484.34205441  26805.1205942\n",
      "  65608.35165289 127870.7265423  181119.80495797 208812.1710825\n",
      " 210019.24896447 194592.73469019 182860.35855902 183252.67869489\n",
      " 196943.62999045 212379.54086067 215719.7435584  204342.4224051\n",
      " 181945.95081833 165152.62691649 161771.26464714 163157.30269455\n",
      " 155636.32492232 133695.92113666 101630.65896306  65526.50998715\n",
      "  37856.6466585   23001.27747032  15053.23729129  11563.57103854\n",
      "  11029.8754604   11154.4494271   14694.04163955  23783.15004201\n",
      "  53570.40367477 102782.94353352 150109.4346079  179254.46082781\n",
      " 189787.19673194 183843.45432118 174411.04079446 169989.73831203\n",
      " 170688.41527905 171133.95384176 166675.65823824 155760.28147288\n",
      " 144179.57004192 137854.91225535 137322.44191868 133303.07252301\n",
      " 119989.20943374  95937.58394792  68056.99735559  42930.94594614\n",
      "  27049.31886858  18626.28816003  13584.47476684  11316.52039566\n",
      "  11029.8754604   11135.22816438  13698.60001311  20065.83127727\n",
      "  41180.6518986   77302.81022471 113597.71490223 140832.17909898\n",
      " 153930.79604399 152378.0324658  143773.72628829 133469.45979086\n",
      " 123407.24411845 115406.87333516 113068.1948704  111824.52433781\n",
      " 110031.57986088 109066.71848161 106988.49836127  98695.93834634\n",
      "  84783.98768116  64678.41899061  44282.28882672  28804.10411853\n",
      "  20130.65998471  15228.61509731  12373.46397507  11125.86398227\n",
      "  11029.8754604   11104.90508348  12493.65842342  16076.46124147\n",
      "  29843.79483704  54655.26128855  80598.80019693 100454.59443537\n",
      " 112161.40313761 111366.02921951 102315.57692026  90081.75020114\n",
      "  78541.52201647  73344.50733237  75708.91205194  79548.61620292\n",
      "  81516.68767075  80497.87736638  76401.49011218  68394.25799018\n",
      "  56597.06208573  42031.79841842  29059.53943617  20359.25931077\n",
      "  15539.76628785  13011.5639983   11721.67722321  11085.02661288\n",
      "  11029.8754604   11050.91990363  11771.11715774  13772.76160569\n",
      "  21441.73086009  36717.98655434  53623.16377407  67512.04822205\n",
      "  75731.5411835   74482.294295    67022.46852027  57099.71066447\n",
      "  50737.01371001  49764.68838857  52503.61204694  56095.23390365\n",
      "  56792.7256831   55001.67824226  51423.88047343  45626.07103164\n",
      "  36864.06679738  27186.01212917  19970.10594955  15447.72414214\n",
      "  13259.89877702  11969.93352304  11309.29741159  11049.00463775\n",
      "  11029.8754604   11038.55208624  11371.47676057  12130.06970799\n",
      "  16145.47887142  25306.43455992  35726.12026498  43790.65783452\n",
      "  48049.61393262  46444.07219162  41064.09086166  36645.76846006\n",
      "  34310.29840156  34706.2118624   36894.73326289  38549.58201291\n",
      "  38373.3912903   36072.14938313  33713.71357646  29722.45067898\n",
      "  24126.52727998  18925.66423834  15252.91208697  13065.37368985\n",
      "  11962.68086067  11381.63708284  11109.14488591  11059.18658552\n",
      "  11029.8754604   11034.90517766  11118.35523848  11493.15326508\n",
      "  13402.23129364  18599.6187317   24465.74523837  28924.66058557\n",
      "  30244.51531916  27849.94896044  24285.78730355  22798.53363397\n",
      "  23111.40231453  24207.18818444  25434.02142881  25463.09872765\n",
      "  24755.15611498  23276.89141513  21984.64263078  19665.85215434\n",
      "  16934.37901232  14586.88328848  12936.73450345  11887.07396485\n",
      "  11391.81538385  11143.91522854  11035.63080789  11030.19774788\n",
      "  11029.8754604   11029.8754604   11060.32255625  11194.05286249\n",
      "  11931.68412933  14246.91211452  17266.3226603   19479.61125147\n",
      "  20039.87101216  18067.00882946  15837.47346908  14544.46432445\n",
      "  14622.47841379  15703.1980496   16267.82612858  16379.33655183\n",
      "  16183.29694973  15707.3542558   15252.47758665  14351.24084132\n",
      "  13456.73256004  12463.84768589  11646.42656458  11265.19076797\n",
      "  11109.04898729  11043.38202214  11029.6410925   11029.8754604\n",
      "  11029.8754604   11029.8754604   11033.20496061  11040.49359534\n",
      "  11176.85804524  11813.57610618  13117.99715585  14167.7728049\n",
      "  14438.7757457   13371.65539883  12482.77336955  11693.31442765\n",
      "  11423.87736783  11551.13054664  11766.39551966  12128.76256443\n",
      "  12326.7987792   12428.64269127  12242.18036235  12001.85142228\n",
      "  11637.43985661  11384.3531806   11127.86039674  11070.70587473\n",
      "  11050.14163251  11031.14598072  11029.85910963  11029.8754604\n",
      "  11029.8754604   11029.8754604   11029.8754604   11030.89508558\n",
      "  11059.40017299  11148.72184838  11433.47758461  11569.75173886\n",
      "  11576.32183467  11412.84940426  11304.25857987  11163.65107036\n",
      "  11127.78200245  11132.78573787  11130.75491833  11187.93395064\n",
      "  11259.37787322  11246.40262602  11167.36373392  11180.09961732\n",
      "  11123.77999421  11069.04628793  11036.90748254  11032.08390622\n",
      "  11030.45889325  11030.72137834  11029.8754604   11029.8754604\n",
      "  11029.8754604   11029.8754604   11029.8754604   11029.8754604\n",
      "  11030.32942135  11031.51313052  11057.63035231  11053.69851299\n",
      "  11058.5585471   11064.28126247  11044.41915458  11034.12639704\n",
      "  11036.0423605   11059.61514062  11029.65468605  11040.99845426\n",
      "  11045.27640912  11034.49384534  11044.40805897  11034.9513687\n",
      "  11030.13261356  11029.61200549  11029.66979013  11029.8754604\n",
      "  11029.8754604   11029.8754604   11029.8754604   11029.8754604\n",
      "  60000.        ]\n",
      "\n",
      "\n",
      "Initialized Loss/Error 0.3195920162418052 9.751666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss: 0.162  Error: 5.575\n",
      "Epoch:  2  Loss: 0.150  Error: 5.107\n",
      "Epoch:  3  Loss: 0.145  Error: 4.963\n",
      "Epoch:  4  Loss: 0.143  Error: 4.837\n",
      "Epoch:  5  Loss: 0.141  Error: 4.768\n",
      "Epoch:  6  Loss: 0.139  Error: 4.690\n",
      "Epoch:  7  Loss: 0.138  Error: 4.662\n",
      "Epoch:  8  Loss: 0.137  Error: 4.618\n",
      "Epoch:  9  Loss: 0.136  Error: 4.608\n",
      "Epoch: 10  Loss: 0.136  Error: 4.578\n",
      "Epoch: 11  Loss: 0.135  Error: 4.568\n",
      "Epoch: 12  Loss: 0.135  Error: 4.548\n",
      "Epoch: 13  Loss: 0.134  Error: 4.530\n",
      "Epoch: 14  Loss: 0.134  Error: 4.512\n",
      "Epoch: 15  Loss: 0.133  Error: 4.507\n",
      "Epoch: 16  Loss: 0.133  Error: 4.478\n",
      "Epoch: 17  Loss: 0.133  Error: 4.460\n",
      "Epoch: 18  Loss: 0.132  Error: 4.447\n",
      "Epoch: 19  Loss: 0.132  Error: 4.435\n",
      "Epoch: 20  Loss: 0.132  Error: 4.413\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "[w, loss, err] = logistic_regression_cd(Xtr, ytr8, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-a0d2faabd9e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss of your logistic classifier over time\n",
    "# The y-axis should be loss, and the x-axis should be the epoch\n",
    "# The plot doesn't need to be pretty, just show the loss going down over time!\n",
    "### ACT 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the error of your logistic classifier over time \n",
    "### ACT 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstr = 'Test  Loss: {0:5.3f}  Error: {1:5.3f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the loss and error of your weight vector on the test data and test labels\n",
    "# (Don't update your weight vector here: this is purely to evaluate it)\n",
    "test_loss, test_err = ### ACT 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pstr.format(test_loss, test_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we visualize w for you\n",
    "# First, we bound the entries of w within 3 standard deviations\n",
    "# (in order to ignore outliers which would mess up the image)\n",
    "image_w = np.maximum(np.minimum(w, 3 * np.std(w)), -3 * np.std(w))\n",
    "# Next, we re-shape image_w into the original 28 by 28 shape of the image\n",
    "image_w = image_w[0:-1].reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-5cb55575aa7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize image_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize image_w\n",
    "plt.axis('off')\n",
    "plt.imshow(image_w, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `ind` is the indices in Xtest of images which are \"8\" but classified incorrectly\n",
    "#   by your classifier\n",
    "ind = (np.argwhere(((yte8 == 1) * (np.matmul(Xte, w) < 0))))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 25 images of \"8\"s which your classifier incorrectly classified as non-8\n",
    "# This kind of visualization can be useful if you're wondering what sorts of images give\n",
    "#   your classifier trouble\n",
    "ncols, nrows = 5, 5\n",
    "fig, axes = plt.subplots(ncols, nrows, figsize=(1.5*ncols, 1.5*nrows))\n",
    "for i in range(ncols * nrows):\n",
    "    ax = axes[i//ncols, i%ncols]\n",
    "    x = X_test[ind[i],:,:].reshape(28,28)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    ax.imshow(x, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that you've completed coordinate descent, you will implement SGD!\n",
    "\n",
    "# A handle is a convenient way to pass in many arguments/specifications\n",
    "# to your function. In this case, the logistic SGD handle holds\n",
    "# parameters such as the gradient function, the loss function, and learning rate.\n",
    "\n",
    "### ACT 20\n",
    "# Fill in the missing parameters in the handle below, and experiment with\n",
    "# different values for them!\n",
    "# For the final submission, use the parameters you found worked best, except set eps = 0.001\n",
    "# (We will not be too picky with your parameters but we do expect them\n",
    "# to be good enough for your SGD to properly learn)\n",
    "# Instead of eta==0 use a real learning rate, (hint: try 0.01, 0.05, and 0.1)\n",
    "# Experiment also with a decreasing learning rate: eta[t] = eta / sqrt(c + t) for eta=1 and c=10\n",
    "def prepare_logistic_sgd_handle():\n",
    "    h = dict()\n",
    "    h['pstr'] = 'Epoch: {0:2d}  Loss: {1:5.3f}  Error: {2:5.3f}'\n",
    "    h['epochs'] = 100\n",
    "    # rather than a single value, eta is an array here, containing the eta for each epoch\n",
    "    h['eta'] = 0.00 * np.ones(h['epochs'])\n",
    "    h['grad'] = logloss_gradient\n",
    "    h['loss'] = logloss\n",
    "    h['error'] = error\n",
    "    ### Adjust this batch size if you wish. A size of 1000 should be fine, though.\n",
    "    h['batch_size'] = 1000\n",
    "    ### You can play with eps, but in your final submission set it to 0.001, so\n",
    "    ### you can achieve the desired accuracy.\n",
    "    h['eps'] = 0.001\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the termination condition (it can be similar to your\n",
    "#   termination condition for coordinate descent)\n",
    "# We recommend you don't terminate when c_loss is greater than p_loss, though\n",
    "def terminate(p_loss, c_loss, eps):\n",
    "    ### ACT 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 22\n",
    "# Implement logistic regression with SGD\n",
    "# h is the handle you defined above\n",
    "def sgd(X, y, h):\n",
    "    loss, error, grad, eta = h['loss'], h['error'], h['grad'], h['eta']\n",
    "    epochs, bs = h['epochs'], h['batch_size']\n",
    "    eps, pstr = h['eps'], h['pstr']\n",
    "    n, d = X.shape\n",
    "    nbs = int(n / bs)\n",
    "    sampler = IndexSampler(nbs)\n",
    "    ### ACT initialize w, losses, errors, and other variables you may need\n",
    "    for e in range(1, epochs * nbs):\n",
    "        # these two lines get a batch of examples and labels for you\n",
    "        head = sampler.sample_new_index(replace=0) * bs\n",
    "        Xt, yt = X[head:head + bs], y[head:head + bs]\n",
    "        ### ACT find stochastic gradient using the functions above with (Xt, yt)\n",
    "        ### ACT update w as appropriate\n",
    "        if e % nbs == 0:\n",
    "            ### ACT update losses and errors\n",
    "            print(pstr.format(e // nbs, losses[-1], errors[-1]))\n",
    "            if terminate(losses[-2], losses[-1], eps): break\n",
    "    print('\\n')\n",
    "    return w, losses, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w_sgd, loss_sgd, error_sgd] = sgd(Xtr, ytr8, prepare_logistic_sgd_handle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss of your SGD classifier over time\n",
    "### ACT 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the error of your SGD classifier over time\n",
    "### ACT 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the error of your SGD classifier and CD classifer on the same graph\n",
    "### ACT 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is similar to above\n",
    "ind = (np.argwhere(((yte8 == 1) * (np.matmul(Xte, w_sgd) < 0))))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 25 images of \"8\"s which your classifier incorrectly classified as non-8\n",
    "ncols, nrows = 5, 5\n",
    "fig, axes = plt.subplots(ncols, nrows, figsize=(1.5*ncols, 1.5*nrows))\n",
    "for i in range(ncols * nrows):\n",
    "    ax = axes[i//ncols, i%ncols]\n",
    "    x = X_test[ind[i],:,:].reshape(28,28)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    ax.imshow(x, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying the digit 1\n",
    "Now, repeat most of the above steps, except using the digit 1 instead. In particular:\n",
    "- Obtain training and test labels (yte1 and ytr1) (Xtr and Xte should remain the same)\n",
    "- Train a new weight vector using either CD or SGD\n",
    "- Visualize your weight vector for \"1\"\n",
    "- Plot the error of your weight vector for 1 over time\n",
    "- Display images of \"1\"s which your classifier incorrectly classified as non-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 26 (this may take multiple cells)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
