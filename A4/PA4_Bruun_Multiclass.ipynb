{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 - Multiclass Learning\n",
    "\n",
    "In this exercise you are required to design, implement, train, and test multiclass learning algorithms for the [MNIST](http://yann.lecun.com/exdb/mnist) dataset of images and the [20 Newsgroups](http://qwone.com/~jason/20Newsgroups) dataset of text news data. You will explore two different approaches to multiclass learning: *Cross Entropy*, a general loss appropriate for multiple classes, and *One-vs-All*, a reduction scheme from binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to have the packages numpy, scikit-learn, and Tensorflow\n",
    "# installed before starting the assignment\n",
    "\n",
    "# import numpy first\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Losses\n",
    "\n",
    "In the case of Cross Entropy, denoted in short as **CE**, the supervision for an example $\\mathbf{x}\\in\\mathbb{R}^d$  is, instead of a label\n",
    "$y\\in[k]\\stackrel{\\tiny \\mathsf{def}}{=}\\{1,\\ldots,k\\}$, a vector $\\mathbf{y}\\in\\Delta^k$ where\n",
    "$$\\Delta^k = \\big\\{\\mathbf{p}=(p_1,\\ldots,p_k)\\, \\big| \\, \\sum_i p_i = 1\n",
    "   \\mbox{ and } \\forall i: p_i \\geq 0 \\big\\} ~.$$\n",
    "$\\Delta^k$ is called the k$^{th}$ dimensional simplex and, informally speaking, it consists of all probability vectors in $\\mathbb{R}^k$. The CE loss for a target probability vector $\\mathbf{y}$ and predicted probability vector $\\mathbf{\\hat{y}}$ is defined as follows,\n",
    "   $$\\ell_{CE}(\\mathbf{y, \\hat{y}}) \\stackrel{\\tiny \\mathsf{def}}{=}\n",
    "     \\sum_{i=1}^k y[i] \\log\\left(\\frac{y[i]}{\\hat{y}[i]}\\right) ~ .$$\n",
    "We use the convention that $0\\log(0)\\stackrel{\\tiny \\mathsf{def}}{=} 0$. To obtain the vector $\\mathbf{\\hat{y}}$ we exponentiate the vector of predictions $\\mathbf{z} = W\\mathbf{x} \\in \\mathbb{R}^k$ where $\\mathbf{\\hat{y}}[i] = \\frac{\\exp({\\mathbf{z}[i]})}{Z}$ for all $i \\in [k]$ and $Z$ ensures that $\\mathbf{\\hat{y}}\\in \\Delta^k $.\n",
    "\n",
    "The One-vs-All, or **OvA**, approach instead uses $k$ logistic binary classifiers: the idea is to pick the label that has been *most confidently* chosen for the given example versus the rest. Formally, the loss can be expressed as follows. Given $(\\mathbf{x},y)\\in\\mathbb{R}^d\\times[k]$ let us define $\\mathbf{\\bar{y}}\\in\\{-1,1\\}^k$ as the vector of labels resulting from the multiclass reduction. That is, $\\mathbf{\\bar{y}}[y]=1$ and for all $j\\neq y$ we set $\\mathbf{\\bar{y}}[j]=-1$. Then the loss for $(\\mathbf{x},y)$, with $\\mathbf{z}$ defined as before, is\n",
    "$$\n",
    "\\ell_{OvA}(\\mathbf{\\bar{y}}, \\mathbf{z}) = \\sum_{j=1}^k \\log\\big(1+ e^{-\\mathbf{\\bar{y}}[j] \\, \\mathbf{z}[j]}\\big) ~ .\n",
    "$$\n",
    "\n",
    "ACT 1: Compute the value of $Z$ explicitly. Propose a method of computing $\\mathbf{\\hat{y}}$ from $\\mathbf{z}$ that is numerically stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "### ACT 1\n",
    "\n",
    "recall $\\mathbf{z} = W\\mathbf{x} \\in \\mathbb{R}^k$\n",
    "\n",
    "   $$ Z = \\sum_{i=1}^k e^{z[i]} $$\n",
    "   \n",
    "   $$ \\mathbf{\\hat{y}}[i] = \\frac{e^{\\mathbf{z}[i]}}{\\sum_{i=1}^k e^{\\mathbf{z}[i]}}$$\n",
    "   \n",
    "Two problems:\n",
    "\n",
    "(1) if all the z-values are <<0, then the summation in the denominator approaches zero. If underflow occurs, where they all get rounded to 0, this will cause an error when dividing by 0.\n",
    "\n",
    "(2) If a single z-value is >>0, the value will be rounded to infinity, overflow will occur\n",
    "\n",
    "Fix this problem by adding a scalar $(c = -max_i(\\mathbf{z}))$ to the z-vector. This does not change the actual calculation:\n",
    "\n",
    "$$ \\frac{e^{\\mathbf{z}[i] + c}}{\\sum_{i=1}^k e^{\\mathbf{z}[i] + c}} = \\frac{e^{\\mathbf{z}[i]}\\cdot e^c }{\\sum_{i=1}^k e^{\\mathbf{z}[i]} \\cdot e^c  }  $$\n",
    "\n",
    "By shifting the z-vector by its max value, it is guaranteed that at least one of the terms will now be 0. And that 0 is the largest number in the z-vector. This fixes the problems above:\n",
    "\n",
    "(1) If one term is 0, then e^0 = 1, and there will never be a 0 in the denominator.\n",
    "\n",
    "(2) Since the largest term is 0, there is no chance of an exceptionally large exponent overflowing.\n",
    "\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X \\in \\mathbb{R}^{n \\times d}$ be the given dataset and $\\mathbf{y} \\in [k]^n$ the corresponding labels. Construct one-hot vectors for each label to have a probability vector from $\\Delta^k$: the one-hot vector for the class $c \\in [k]$ if defined as $\\mathbf{1}_c \\in \\Delta^k$ which is a vector of zeros except for coordinate $c$ where it is $1$. For example, suppose $k=4$ and $c=2$, then $\\mathbf{1}_c$ is $(0, 1, 0, 0)$. When needed we will\n",
    "be smoothing the one-hot vectors for numerical purposes. The $\\epsilon$-smoothed one-hot vector for class $c$ is given by $(1-\\epsilon) \\mathbf{1}_c + \\epsilon \\mathbf{\\frac1k}$ where $\\frac1k \\in \\Delta^k$ is the uniform probability vector, namely $\\mathbf{\\frac1k} = (\\frac1k, \\frac1k, \\ldots, \\frac1k)$.\n",
    "\n",
    "ACT 2: Implement the `one_hot` function that given $\\mathbf{y} \\in [k]^n$ data of labels returns $Y \\in \\mathbb{R}^{n \\times k}$ matrix of one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given y vector of labels, return matrix Y whose i-th row is the one-hot vector of label y_i \n",
    "# If eps is not 0, then need to compute the smoothed one-hot vectors instead\n",
    "# If k is 0, then need to infer its value instead\n",
    "def one_hot(y, k=0, eps=0):\n",
    "    ### ACT 2\n",
    "    #print(\"one-hot function\")\n",
    "    \n",
    "    n = len(y)\n",
    "\n",
    "    if not k:\n",
    "        labels,c = np.unique(y,return_counts=True) #ordered list of unique labels\n",
    "        k = len(labels) #infer the number of classes based on how many unique labels there are\n",
    "        Y = np.zeros((n,k))\n",
    "        for i,y_i in enumerate(y):\n",
    "            Y[i][np.where(labels == y_i)] = 1  #place into column corresponding to location in the ordered list    \n",
    "    else:\n",
    "        Y = np.zeros((n,k))\n",
    "        for i,y_i in enumerate(y):\n",
    "            Y[i][y_i] = 1    \n",
    "    \n",
    "    if eps: \n",
    "        Y_bool = Y!=0\n",
    "        Y[Y_bool] += -eps + eps/k\n",
    "        Y[~Y_bool] += eps/k\n",
    "   \n",
    "    return Y\n",
    "  \n",
    "\n",
    "#ASSERTS\n",
    "y_assert = np.array([9, 1, 0, 2, 1, 2])\n",
    "\n",
    "Y_out = one_hot(y_assert)\n",
    "assert(np.shape(Y_out) == (6,4))\n",
    "assert(Y_out[0][3] == 1)\n",
    "assert(sum(sum(Y_out)) == 6)\n",
    "#print(Y_out)\n",
    "\n",
    "Y_out = one_hot(y_assert, 0, 0.1)\n",
    "assert(Y_out[1][1] == 0.925)\n",
    "assert(sum(sum(Y_out)) == 6.0)\n",
    "#print(Y_out)\n",
    "\n",
    "Y_out = one_hot(y_assert, 10, 0.1)\n",
    "assert(np.shape(Y_out) == (6,10))\n",
    "assert(np.isclose(sum(sum(Y_out)),6.0))\n",
    "assert(Y_out[0][5] == 0.01)\n",
    "assert(Y_out[1][1] == 0.91)\n",
    "#print(Y_out)\n",
    "\n",
    "y_assert = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 0, 1, 1, 1, 2, 2])\n",
    "Y_out = one_hot(y_assert,[], 0.1)\n",
    "#print(Y_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will first implement an abstract class for losses in general that has all the methods implemented except computing the loss value and the gradient. Then each of CE and OvA classes will implement their specific loss functions, and gradients separately. *Feel free to add your own methods to the class skeletons provided.*\n",
    "\n",
    "It is worth to note that label prediction in both cases is done by predicting the coordinate with the largest value in the vector $\\mathbf{z}$ as the label. In order to prevent the parameters from blowing up during training, we will implement methods `RowNorms` and `Project` to compute the norms of the rows of $W \\in \\mathbb{R}^{k \\times d}$ and their projections onto the sphere of radius $r$.\n",
    "\n",
    "ACT 3: Given a vector $\\mathbf{v} \\in \\mathbb{R}^d$, compute its projection onto the sphere $S_r = \\{ \\mathbf{u} \\in \\mathbb{R}^d \\, : \\, \\| \\mathbf{u} \\| \\leq r \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "### ACT 3\n",
    "\n",
    "if $ ||\\mathbf{v}|| > r $ then the projection on sphere is found when $\\mathbf{v}$ is projected onto $\\mathbf{u}$, which is a vector from the center to the surface of the sphere chosen to minimize $||\\mathbf{v} - \\mathbf{u}||$.\n",
    "\n",
    "This minimum is achieved by the vector that is in the same direction as $\\mathbf{v}$, hence the projection on the sphere is just a scaling of the ratios of these vectors.\n",
    "\n",
    "$$ \\mathbf{v}_{proj}= \\frac{r}{||\\mathbf{v}||} \\cdot \\mathbf{v} $$\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class GenericLoss:\n",
    "    def __init__(self, name=\"\", dims = [], W0 = []):\n",
    "        self.name = name\n",
    "        assert dims != [] or len(W0) != 0, 'Must set dims or W0'\n",
    "        \n",
    "        if len(W0) == 0:\n",
    "            self.W = np.zeros(dims)\n",
    "        else:\n",
    "            self.W = W0\n",
    "            \n",
    "        self.k, self.d = self.W.shape\n",
    "        \n",
    "        #Added variables\n",
    "        self.norms = np.zeros(self.k)\n",
    "        self.zs = []\n",
    "        self.y_label_pred = []\n",
    "        self.error = []\n",
    "    \n",
    "    # set the parameter matrix to the given W\n",
    "    def Set(self, W):\n",
    "        ### ACT 4\n",
    "        self.W = copy.deepcopy(W)\n",
    "    \n",
    "    # get the parameter matrix of the instance\n",
    "    def Get(self):\n",
    "        ### ACT 5\n",
    "        return self.W\n",
    "    \n",
    "    # update the parameter matrix by adding given dW\n",
    "    def Update(self, dW):\n",
    "        ### ACT 6\n",
    "        self.W += dW\n",
    "    \n",
    "    # compute the norms of the rows of the parameter matrix\n",
    "    def RowNorms(self):\n",
    "        ### ACT 7\n",
    "        self.norms = np.linalg.norm(self.W, axis=1)\n",
    "        \n",
    "    # compute the projection of each of the rows to the sphere of radius rad\n",
    "    def Project(self, rad):\n",
    "        ### ACT 8\n",
    "        self.RowNorms()\n",
    "        \n",
    "        scale = rad/self.norms\n",
    "        scale = np.where(scale>1,1,scale)\n",
    "        \n",
    "        self.W = (self.W.T*scale).T\n",
    "\n",
    "    # compute the numerical predictions (z values) given a data matrix X\n",
    "    def Predict(self, X):\n",
    "        ### ACT 9\n",
    "        self.zs = np.dot(self.W,X.T).T\n",
    "\n",
    "    # compute the label predictions given a data matrix X\n",
    "    def PredictLabels(self, X):\n",
    "        ### ACT 10\n",
    "        self.Predict(X) # Calculate z-values\n",
    "        self.y_label_pred = np.argmax(self.zs,axis=1)\n",
    "        \n",
    "    # compute the classification error given a data matrix X, and label vector y\n",
    "    def Error(self, X, y):\n",
    "        ### ACT 11\n",
    "        self.PredictLabels(X)\n",
    "        \n",
    "        n = len(y)\n",
    "        n_true = np.count_nonzero(y == self.y_label_pred)\n",
    "        \n",
    "        self.error = (n - n_true)/n\n",
    "        \n",
    "    def Loss(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def Gradient(self, X, y):\n",
    "        pass\n",
    "    \n",
    "### ASSERTS\n",
    "w = np.array([[1, 0, 0, 1], [2, 3, 1, 1], [1, 2, 2, 2]])\n",
    "L = GenericLoss(\"assert_test\",W0 = w)\n",
    "assert(np.all(L.W == w))\n",
    "L = GenericLoss(\"assert_test\",(3,4),[])\n",
    "\n",
    "#Set Function\n",
    "\n",
    "L.Set(w)\n",
    "assert(np.all(L.W == w))\n",
    "\n",
    "#Update Function\n",
    "L.Update(np.array([[0, 0, 1, 0], [0, 1, 1, 0], [0, 0, 0, 1]]))\n",
    "assert(L.W[0][2] == (w[0][2] + 1))\n",
    "\n",
    "#Norm Function\n",
    "L.Set(w)\n",
    "L.RowNorms()\n",
    "assert(L.norms[0] == np.sqrt(2))\n",
    "\n",
    "L.Update(np.array([[0, 0, 1, 0], [0, 1, 1, 0], [0, 0, 0, 1]]))\n",
    "L.RowNorms()\n",
    "assert(L.norms[1] == np.sqrt(25))\n",
    "\n",
    "#Project Function\n",
    "rad = 3.0\n",
    "L.Set(w)\n",
    "L.Project(rad)\n",
    "L.RowNorms()\n",
    "assert(L.norms[0] == np.sqrt(2))\n",
    "assert(L.norms[1] == rad)\n",
    "\n",
    "#Predict Function\n",
    "L.Set(w)\n",
    "X = np.array([[1, 1, 1, 1], [0, 0, 1, 3]])\n",
    "L.Predict(X)\n",
    "assert(np.shape(L.zs) == (2,3))\n",
    "assert(L.zs[0][0] == 2)\n",
    "\n",
    "#Predict Labels Function\n",
    "L.Set(w)\n",
    "X = np.array([[1, 1, 1, 2], [0, 0, 1, 3], [3, 0, -1, 1]])\n",
    "L.PredictLabels(X)\n",
    "assert(L.y_label_pred[1] == 2)\n",
    "\n",
    "#Errors Function\n",
    "L.Set(w)\n",
    "X = np.array([[1, 1, 1, 2], [0, 0, 1, 3], [3, 0, -1, 1]])\n",
    "y = np.array([2,1,1])\n",
    "L.Error(X,y)\n",
    "assert(L.error == 1/3)\n",
    "\n",
    "\n",
    "del(L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the specific losses, you simply need to compute the loss value itself given $X \\in \\mathbb{R}^{n \\times d}, \\mathbf{y} \\in [k]^n$ and the parameter matrix $W \\in \\mathbb{R}^{k \\times d}$ as well as the gradient of that loss with respect to $W$. Let us first express $\\ell^{\\mathsf{CE}}$ and $\\ell^{\\mathsf{OvA}}$ in terms of $X, \\mathbf{y}, W$.\n",
    "\n",
    "$$\\ell^{\\mathsf{CE}}(X, \\mathbf{y}, W) = \\frac{1}{n} \\sum_{i=1}^n \\ell_{CE}(\\mathbf{y}_i, \\hat{\\mathbf{y}}_i), \\quad \\ell^{\\mathsf{OvA}}(X, \\mathbf{y}, W) = \\frac{1}{n} \\sum_{i=1}^n \\ell_{OvA}(\\bar{\\mathbf{y}}_i, \\mathbf{z}_i) ~ .$$\n",
    "\n",
    "where for each $i \\in [n]$ the vectors $\\mathbf{y}_i$ are the rows of $Y =$ `one_hot`$(\\mathbf{y}) \\in \\mathbb{R}^{n \\times k}$, $\\bar{\\mathbf{y}}_i$ are the rows of $\\bar{Y} = (2Y - 1) \\in \\mathbb{R}^{n \\times k}$, $\\mathbf{z}_i$ are the rows of $X W^{\\top} \\in \\mathbb{R}^{n \\times d}$, and $\\hat{\\mathbf{y}}_i = \\exp(\\mathbf{z}_i)/Z_i$ with $Z_i$ making sure $\\hat{\\mathbf{y}}_i \\in \\Delta^k$. The gradient computation can be done step-by-step using the chain rule.\n",
    "\n",
    "ACT 12: Compute the gradient of $\\ell^{\\mathsf{CE}}$ w.r.t. $W$, given by $\\nabla_W \\ell^{\\mathsf{CE}}$.\n",
    "\n",
    "ACT 13: Compute the gradient of $\\ell^{\\mathsf{OvA}}$ w.r.t. $W$, given by $\\nabla_W \\ell^{\\mathsf{OvA}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACT 12\n",
    "\n",
    "For a single example, the gradient is computed as shown below. Find these three derivatives based on the chain rule. \n",
    "\n",
    "   $$\\nabla_W \\ell_{\\mathsf{CE}} = \\frac{\\partial\\ell_{CE}(\\mathbf{y, \\hat{y}})}{\\partial\\mathbf{\\hat{y}}} \\cdot \\frac{\\partial\\mathbf{\\hat{y}}}{\\partial \\mathbf{z}} \\cdot \\frac{\\partial\\mathbf{z}}{\\partial \\mathbf{W}} $$\n",
    "   \n",
    "In the following derivation, the summations and square brackets indicate performing the operation on every entry in the vector $\\in \\mathbb{R}^{k}$. The derivation is done assuming just one entry, but later this summation can be replaced with the vector form.\n",
    "\n",
    "1st term:\n",
    "\n",
    "   $$\\ell_{CE}(\\mathbf{y, \\hat{y}}) =\n",
    "     \\sum_{i=1}^k y[i] \\log\\left(\\frac{y[i]}{\\hat{y}[i]}\\right) = \\sum_{i=1}^k y[i] \\log(y[i]) - y[i]\\log(\\hat{y}[i])$$\n",
    "     \n",
    "$$ \\frac{\\partial\\ell_{CE}(\\mathbf{y, \\hat{y}})}{\\partial\\mathbf{\\hat{y}}} = \\sum_{i=1}^k - \\frac{y[i]}{\\hat{y}[i]}$$\n",
    "\n",
    "2nd term:\n",
    "\n",
    "$$ \\mathbf{\\hat{y}}[i] = \\frac{e^{{z}[i]}}{\\sum_{i=1}^k e^{{z}[i]}} = \\frac{e^{z[i]}}{Z} = \\frac{e^{z[i]}}{C + e^{z[i]}} \\cdot \\frac{e^{-z[i]}}{e^{-z[i]}} = \\frac{1}{1+Ce^{-z[i]}}$$\n",
    "\n",
    "$$ \\frac{\\partial\\mathbf{\\hat{y}}[i]}{\\partial \\mathbf{z}[i]} = \\frac{Ce^{z[i]}}{(1 + Ce^{-z[i]})^2} = \\frac{Ce^{z[i]}}{(e^{z[i]} + C)^2} = \\frac{(Z-e^{z[i]})e^{z[i]}}{Z^2} = \\frac{e^{z[i]}}{Z} \\cdot \\frac{Z - e^{z[i]}}{Z} = \\hat{y}[i]\\cdot(1 - \\hat{y}[i])$$\n",
    "\n",
    "3rd term:\n",
    "\n",
    "$$ \\frac{\\partial z[i]}{\\partial W} = \\mathbf{x}$$\n",
    "   \n",
    "the $z[i]$ entry corresonds to $W_{i*}$ row times $\\mathbf{x}$ (the feature values in the example). Therefore, the partial over the whole $W \\in \\mathbb{R}^{k \\times d}$ corresponds to just $\\mathbf{x} \\in \\mathbb{R}^{d}$. This is the case for each term in $z \\in \\mathbb{R}^{k}$.\n",
    "\n",
    "Putting it together (SINGLE EXAMPLE), results in a $\\mathbf{G} \\in \\mathbb{R}^{k\\times d}$ gradient matrix:\n",
    "\n",
    "$$ \\nabla_W \\ell_{\\mathsf{CE}} = \\sum_{i=1}^k - \\frac{y[i]}{\\hat{y}[i]} \\left(\\hat{y}[i]\\cdot(1 - \\hat{y}[i])\\right) \\cdot \\mathbf{x} =  \\sum_{i=1}^k - y[i]\\left(1 - \\hat{y}[i]\\right) \\cdot \\mathbf{x} = -\\mathbf{y}\\left(1 - \\mathbf{\\hat{y}}\\right) \\times \\mathbf{x}$$\n",
    "\n",
    "Putting it together (ALL EXAMPLES):\n",
    "\n",
    "$$ \\nabla_W \\ell^{\\mathsf{CE}} = \\frac{1}{n} \\sum_{i=1}^n -\\mathbf{y}_i\\left(1 - \\mathbf{\\hat{y}}_i\\right) \\times \\mathbf{x}_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACT 13\n",
    "\n",
    "For a single example, the gradient is computed as shown below. Find these three derivatives based on the chain rule.\n",
    "\n",
    "   $$\\nabla_W \\ell_{\\mathsf{OvA}} = \\frac{\\partial\\ell_{Ova}(\\mathbf{\\bar{y}, z})}{\\partial\\mathbf{z}} \\cdot \\frac{\\partial\\mathbf{z}}{\\partial \\mathbf{W}} $$\n",
    "   \n",
    "1st term:\n",
    "\n",
    "   $$ \\ell_{OvA}(\\mathbf{\\bar{y}}, \\mathbf{z}) = \\sum_{j=1}^k \\log\\big(1+ e^{-\\mathbf{\\bar{y}}[j] \\, \\mathbf{z}[j]}\\big) $$\n",
    "   \n",
    "   $$ \\frac{\\partial\\ell_{OvA}(\\mathbf{\\bar{y}}, \\mathbf{z})}{\\partial\\mathbf{z}} = \\sum_{i=1}^k - \\frac{\\bar{y}[i] e^{-\\bar{y}[i] z[i]}}{1 + e^{-\\bar{y}[i] z[i]}} \\cdot \\frac{e^{\\bar{y}[i] z[i]}}{e^{\\bar{y}[i] z[i]}} =  \\sum_{i=1}^k - \\frac{\\bar{y}[i]}{1 + e^{\\bar{y}[i] z[i]}}$$\n",
    "   \n",
    "2nd term:\n",
    "\n",
    "$$ \\frac{\\partial z[i]}{\\partial W} = \\mathbf{x}$$\n",
    "\n",
    "Putting it together (SINGLE EXAMPLE), results in a $\\mathbf{G} \\in \\mathbb{R}^{k\\times d}$ gradient matrix:\n",
    "\n",
    "$$ \\nabla_W \\ell_{\\mathsf{OvA}} = \\sum_{i=1}^k - \\frac{\\bar{y}[i]}{1 + e^{\\bar{y}[i] z[i]}} \\cdot \\mathbf{x}$$\n",
    "\n",
    "to simplify the expression $ \\frac{\\bar{y}[i]}{1 + e^{\\bar{y}[i] z[i]}} = \\mathbf{c}[i]$, in vector form:\n",
    "\n",
    "$$ \\nabla_W \\ell_{\\mathsf{OvA}} =-\\mathbf{c} \\times \\mathbf{x} $$\n",
    "\n",
    "Putting it together (ALL EXAMPLES):\n",
    "\n",
    "$$ \\nabla_W \\ell^{\\mathsf{OvA}} = \\frac{1}{n} \\sum_{i=1}^n -\\mathbf{c}_i \\times \\mathbf{x}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]] [1 2 1 1 1]\n",
      "[[1 2 1 1 1]\n",
      " [2 4 2 2 2]\n",
      " [3 6 3 3 3]\n",
      " [4 8 4 4 4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.array([1, 2, 3, 4]).reshape(-1,1)\n",
    "x = np.array([1, 2, 1, 1, 1])\n",
    "\n",
    "print(c,x)\n",
    "print(c*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(GenericLoss):\n",
    "\n",
    "    # In CE, the one-hot vectors should be smoothed with the given eps parameter.\n",
    "    def __init__(self, eps=1e-6, name=\"Cross Entropy\", dims = [], W0 = []):\n",
    "        super().__init__(name, dims, W0)\n",
    "        self.eps = eps\n",
    "        self.Y = []\n",
    "        self.y_hat = []\n",
    "        \n",
    "    # compute the CE loss as defined above.\n",
    "    def Loss(self, X, y):\n",
    "        ### ACT 14\n",
    "        self.Y = one_hot(y,[],self.eps)\n",
    "        \n",
    "        self.Predict(X) # Calc z-values\n",
    "        self.Stabilize_z()\n",
    "        self.Calc_y_hat()\n",
    "        \n",
    "        a = np.log(self.Y/self.y_hat)\n",
    "        loss = np.sum(self.Y*a,axis=1) #For each example, sum over each class\n",
    "        \n",
    "        return np.average(loss)\n",
    "        \n",
    "    # compute the gradient w.r.t. W as done in ACT 12.\n",
    "    def Gradient(self, X, y):\n",
    "        ### ACT 15\n",
    "        pass\n",
    "    \n",
    "    ###ADDITIONAL METHODS FOR CE####\n",
    "    # Update the z-vector as per ACT 1 (for each example)\n",
    "    def Stabilize_z(self):\n",
    "        z_max = np.max(self.zs,axis=1)\n",
    "        self.zs = self.zs-z_max.reshape(-1,1)\n",
    "    \n",
    "    # Calculate the y-prediction vector (for each example)\n",
    "    def Calc_y_hat(self):\n",
    "        self.y_hat = np.exp(self.zs)\n",
    "        y_hat_sum = np.sum(self.y_hat, axis=1)\n",
    "        self.y_hat = self.y_hat/y_hat_sum.reshape(-1,1) \n",
    "    ###################################\n",
    "\n",
    "\n",
    "class LogisticOvA(GenericLoss):\n",
    "    \n",
    "    def __init__(self, name=\"One vs All\", dims = [], W0 = []):\n",
    "        super().__init__(name, dims, W0)\n",
    "        self.Y_bar = []\n",
    "    \n",
    "    # compute the OvA loss as defined above.\n",
    "    def Loss(self, X, y):\n",
    "        ### ACT 16\n",
    "        Y = one_hot(y,[],[])\n",
    "        self.Y_bar = (2*Y - 1)\n",
    "        \n",
    "        self.Predict(X) # Calc z-values\n",
    "        \n",
    "        a = 1 + np.exp(-self.Y_bar*self.zs) \n",
    "        loss = np.sum(np.log(a),axis=1) #For each example, sum over each class\n",
    "        \n",
    "        return np.average(loss)\n",
    "          \n",
    "    # compute the gradient w.r.t. W as done in ACT 13.\n",
    "    def Gradient(self, X, y):\n",
    "        ### ACT 17\n",
    "        pass\n",
    "        \n",
    "### ASSERTS\n",
    "w = np.array([[1, 0, 0, 1], [2, 3, 1, 1], [1, 2, 2, 2]])\n",
    "X = np.array([[1, 1, 1, 2], [0, 0, 1, 3], [3, 0, -1, 1]])\n",
    "y = np.array([2,0,1])\n",
    "\n",
    "### FOR CE\n",
    "a = CrossEntropy(W0 = w)\n",
    "\n",
    "# Stabilize Function\n",
    "a.Predict(X)\n",
    "a.Stabilize_z()\n",
    "assert(a.zs[0][2] == 0)\n",
    "assert(a.zs[2][2] == -3)\n",
    "\n",
    "# Calulate y_hat Function\n",
    "a.Calc_y_hat()\n",
    "assert(np.isclose(a.y_hat[0][2] - 0.72973621, 0))\n",
    "assert(np.isclose(a.y_hat[1][1] - 0.01786798, 0))\n",
    "assert(np.sum(a.y_hat,axis=1)[0] == 1)\n",
    "\n",
    "# Loss Function\n",
    "avg_loss = a.Loss(X,y)\n",
    "assert(np.isclose(avg_loss - 1.836544413, 0))\n",
    "\n",
    "del(a)\n",
    "\n",
    "\n",
    "### FOR OvA\n",
    "a = LogisticOvA(W0 = w)\n",
    "a.Set(w)\n",
    "\n",
    "avg_loss = a.Loss(X,y)\n",
    "assert(np.isclose(avg_loss - 10.06177727, 0))\n",
    "\n",
    "del(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the two loss classes, we will test them the following way:\n",
    "\n",
    "(i) the loss of random labels should be larger than the loss of the assigned labels (the result of `PredictLabels`);\n",
    "\n",
    "(ii) the gradient with random labels should have a bigger norm than that with the assigned labels;\n",
    "\n",
    "(iii) the gradient norm should decrease after a single small gradient step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-299-ac6b5d98beb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mLossTested\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mTestLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLossTested\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CE Test Passed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-299-ac6b5d98beb7>\u001b[0m in \u001b[0;36mTestLoss\u001b[0;34m(loss_tested)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# assert that the loss value with the assigned labels y is smaller than that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# with labels uniformly random from the interval [0, k-1].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredictLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;31m### ACT 18: loss value on X with assigned labels y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;31m### ACT 19: loss value on X with random labels y_rand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lt' is not defined"
     ]
    }
   ],
   "source": [
    "# assert the 3 points above for the loss class given by loss_tested\n",
    "# after implementing make sure both CrossEntropy, LogisticOvA pass all tests\n",
    "def TestLoss(loss_tested):\n",
    "    n, d, k, tests = 100, 10, 7, 1000\n",
    "    test_loss = loss_tested(W0=[], dims=(k, d))\n",
    "    print(test_loss.name)\n",
    "\n",
    "    \n",
    "    # the number of tests is given by tests.\n",
    "    # for each test, generate random Gaussian matrices X, W of appropriate sizes.\n",
    "    for _ in range(tests):\n",
    "        X, W = np.random.randn(n, d), np.random.randn(k, d)\n",
    "        test_loss.Set(W)\n",
    "    \n",
    "        # assert that the loss value with the assigned labels y is smaller than that\n",
    "        # with labels uniformly random from the interval [0, k-1].\n",
    "        y, y_rand = lt.PredictLabels(X), np.random.randint(0, k, n)\n",
    "        loss1 = 0### ACT 18: loss value on X with assigned labels y\n",
    "        loss2 = 0### ACT 19: loss value on X with random labels y_rand\n",
    "        assert loss1 < loss2, \"Loss test failed (%f >= %f)\" % (loss1, loss2)\n",
    "    \n",
    "        # assert that the gradient norm with the assigned labels is smaller than that\n",
    "        # with labels uniformly random from the interval [0, k-1].\n",
    "        grad1 = test_loss.Gradient(X, y)\n",
    "        norm_grad1 = 0### ACT 20: norm of the gradient with X and assigned labels y\n",
    "        norm_grad2 = 0### ACT 21: norm of the gradient with X and random labels y_rand\n",
    "        assert norm_grad1 < norm_grad2, \"Gradient norm test failed (%f >= %f)\" % (norm_grad1, norm_grad2)\n",
    "        \n",
    "        # assert that after making a single gradient step (in the opposite direction)\n",
    "        # the gradient norm decreases (choose a small step size).\n",
    "        test_loss.Update(-0.01 * grad1)\n",
    "        norm_grad3 = 0### ACT 22: norm of the gradient with X and y after making a single gradient step\n",
    "        assert norm_grad3 < norm_grad1, \"Gradient step test failed (%f >= %f)\" % (norm_grad3, norm_grad1)\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "LossTested = CrossEntropy\n",
    "if TestLoss(LossTested):\n",
    "    print('CE Test Passed')\n",
    "    \n",
    "LossTested = LogisticOvA\n",
    "if TestLoss(LossTested):\n",
    "    print('Logistic OvA Test Passed')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are methods from previous assignments for data processing and training. No need to reimplement these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a mini-batch w/ or w/o replacement\n",
    "from numpy.random import randint\n",
    "from numpy.random import permutation\n",
    "class IndexSampler:\n",
    "    def __init__(self, d):\n",
    "        self.d = d\n",
    "        self.prm = None\n",
    "    \n",
    "    def sample_new_index(self, replace=0):\n",
    "        if replace:\n",
    "            return randint(self.d)\n",
    "        if self.prm is None:\n",
    "            self.prm = permutation(self.d)\n",
    "            self.head = 0\n",
    "        ind = self.prm[self.head]\n",
    "        self.head += 1\n",
    "        if self.head == self.d:\n",
    "            self.head = 0\n",
    "            self.prm = None\n",
    "        return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector of learning-rate values. Mode can be: 'fixed_t', 'linear_t', 'sqrt_t'\n",
    "# Internal shift_t parameter can/should be changed during experiments.\n",
    "def learning_rate_schedule(eta0, epochs, mode):\n",
    "    base_t = 10.0\n",
    "    if mode == 'fixed_t':\n",
    "        return eta0 * np.ones(epochs)\n",
    "    if mode == 'sqrt_t':\n",
    "        return eta0 * np.ones(epochs) / (base_t + np.sqrt(np.arange(epochs)))\n",
    "    if mode == 'linear_t':\n",
    "        return eta0 * np.ones(epochs) / (base_t + np.arange(epochs))\n",
    "    print('invalid mode for learning rate schedule: %s' % mode)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD with general loss class. h is the handle defined above.\n",
    "def SGD(X, y, Loss, params):\n",
    "    h = params\n",
    "    eps, pstr, rad, replace = h['eps'], h['pstr'], h['rad'], ['replace']\n",
    "    eta0, epochs, bs, lrmode = h['eta0'], h['epochs'], h['batch_size'], h['lr_mode']\n",
    "    n, d = X.shape\n",
    "    nbs = int(n / bs)\n",
    "    k = max(y) + 1\n",
    "    ls = Loss(W0=[], dims=(k, d))\n",
    "    eta_t = learning_rate_schedule(eta0, epochs, lrmode)\n",
    "    losses = [ls.Loss(X, y)]\n",
    "    errors = [ls.Error(X, y)]\n",
    "    sampler = IndexSampler(nbs)\n",
    "    for e in range(1, epochs * nbs):\n",
    "        head = sampler.sample_new_index(replace) * bs\n",
    "        Xt, yt = X[head:head + bs], y[head:head + bs]\n",
    "        gw = ls.Gradient(Xt, yt)\n",
    "        ls.Update(-eta_t[e // nbs] * gw)\n",
    "        if rad > 0: ls.Project(rad)\n",
    "        if e % nbs == 0:\n",
    "            losses.append(ls.Loss(X, y))\n",
    "            errors.append(ls.Error(X, y))\n",
    "        if (e % (nbs * 10)) == 0:\n",
    "            print(pstr.format(e // nbs, losses[-1], errors[-1]))\n",
    "    return ls, losses, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib and get the mnist dataset from tensorflow.keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist as keras_mnist\n",
    "(X_train, y_train), (X_test, y_test) = keras_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data by subtracting the mean and dividing by std\n",
    "def normalize(X, bias=0):\n",
    "    n, d = X.shape\n",
    "    m = np.mean(X, axis=1).reshape(n, 1) * np.ones((1, d))\n",
    "    s = np.std(X, axis=1).reshape(n, 1) * np.ones((1, d))\n",
    "    Xn = (X - m) / s\n",
    "    if bias != 0:\n",
    "        Xn = np.hstack((Xn, bias * np.ones((n, 1))))\n",
    "    return Xn\n",
    "\n",
    "# flatten the images into d-dimensional vectors for training\n",
    "def flatten_images(X):\n",
    "    s = X.shape\n",
    "    n = s[0]\n",
    "    d = np.prod(s[1:])\n",
    "    return X.reshape(n, d)\n",
    "\n",
    "Xtr = normalize(flatten_images(X_train), bias = 1)\n",
    "Xte = normalize(flatten_images(X_test), bias = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 23\n",
    "# run SGD with CrossEntropy loss on the MNIST training data\n",
    "# batch size should be 1000, sampling with no replacement\n",
    "# number of epochs should be 500, sphere radius for W is 10.0\n",
    "# learning rate mode is sqrt_t with eta=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 24\n",
    "# compute test error and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 25\n",
    "# plot loss vs epochs, separately plot error vs epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 26\n",
    "# using test data construct a confusion matrix C that is 10x10\n",
    "# C[i][j] indicates pct digit i was classified as j\n",
    "# assert that each digit is most likely to be correctly classified\n",
    "assert np.array_equal(np.argmax(C, axis=1), np.arange(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 27\n",
    "# perform ACT 23-25 with LogisticOvA instead of CrossEntropy\n",
    "# compare your results, conclude in 1 sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing for 20 newsgroups datasets already done for you.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from collections import Counter\n",
    "\n",
    "def get_20newsgroups_data():\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "    newsgroups_test  = fetch_20newsgroups(subset='test')\n",
    "    return newsgroups_train, newsgroups_test\n",
    "\n",
    "def construct_vocabulary(data, vs):\n",
    "    vocab = Counter()\n",
    "    for text in data:\n",
    "        for word in text.split(' '):\n",
    "            vocab[word.lower()] += 1\n",
    "    word2index = dict(vocab.most_common(vs))\n",
    "    i = 0\n",
    "    for k in word2index.keys():\n",
    "        word2index[k] = i\n",
    "        i += 1\n",
    "    return word2index\n",
    "\n",
    "def text_to_vec(data, vocab):\n",
    "    def norm_rows(M):\n",
    "        return np.sqrt(np.sum(M * M, axis=1, keepdims=True))\n",
    "    def project_rows(M, r):\n",
    "        return M * np.minimum(r / norm_rows(M), 1.0)\n",
    "    n = len(data)\n",
    "    d = len(vocab)\n",
    "    X = np.zeros((n, d))\n",
    "    i = 0\n",
    "    for text in data:\n",
    "        for word in text.split(' '):\n",
    "            if word.lower() in vocab:\n",
    "                X[i, vocab[word.lower()]] += 1.0\n",
    "        i += 1\n",
    "    # Convert to log-frequencies and normalize to have ||X[i,*]||=1\n",
    "    X = project_rows(np.log(X + 1.0), 1.0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsize = 1000\n",
    "train, test = get_20newsgroups_data()\n",
    "vocab = construct_vocabulary(train.data, vsize)\n",
    "Xtr = text_to_vec(train.data, vocab)\n",
    "Xte = text_to_vec(test.data, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will perform the same experiments with this dataset\n",
    "### ACT 28\n",
    "# perform ACT 23-27 for the 20newsgroups dataset\n",
    "# all parameters for training should be the same except:\n",
    "# we will start with a much larger learning rate eta=1000.0\n",
    "# and a sphere radius 40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 29\n",
    "# after training, testing, and plotting make sure to compute the\n",
    "# row norms of the final parameters (mean and std), and conclude \n",
    "# in one sentence whether the results were as you expected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
